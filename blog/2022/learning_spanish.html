<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><link rel="alternate" type="application/rss+xml" title="sontek&#x27;s Engineering Blog" href="/rss.xml"/><title>How to speak spanish like a colombian drug lord!</title><meta name="next-head-count" content="4"/><link rel="preload" href="/_next/static/css/c9439d4f00292099.css" as="style"/><link rel="stylesheet" href="/_next/static/css/c9439d4f00292099.css" data-n-g=""/><link rel="preload" href="/_next/static/css/e64130af1ba1a327.css" as="style"/><link rel="stylesheet" href="/_next/static/css/e64130af1ba1a327.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-69bfa6990bb9e155.js" defer=""></script><script src="/_next/static/chunks/framework-4556c45dd113b893.js" defer=""></script><script src="/_next/static/chunks/main-2a9c662ddd7329fb.js" defer=""></script><script src="/_next/static/chunks/pages/_app-e535547a4fdaa691.js" defer=""></script><script src="/_next/static/chunks/996-9e3c12b77542c098.js" defer=""></script><script src="/_next/static/chunks/790-a3aa35eb62ec874e.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5B...id%5D-c6e585d86e012327.js" defer=""></script><script src="/_next/static/AoGbC8xlMEkyyIwWs3twS/_buildManifest.js" defer=""></script><script src="/_next/static/AoGbC8xlMEkyyIwWs3twS/_ssgManifest.js" defer=""></script><script src="/_next/static/AoGbC8xlMEkyyIwWs3twS/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><div><div class="grid"><div class="col"><header id="banner" class="body"><h1><a href="/">sontek.net</a></h1></header></div><div class="col menu"><nav><ul><li><a href="/">Home</a></li><li><a href="/blog">Blog</a></li><li><a href="/resume">Resume</a></li><li><a href="/about">About</a></li></ul></nav></div></div><div class="container"><article class="blog_article__NoEy4"><h1 class="util_headingXl__knZ_h">How to speak spanish like a colombian drug lord!</h1><div class="util_lightText__xcqUE"><time dateTime="2022-04-30T00:00:00Z">April 30, 2022</time></div><div><p>I've been living in Puerto Rico for 4 years but two of those have been COVID and so I haven't been able to practice Spanish as much as I'd like. So to speed up my learning I've decided I want to watch a lot of spanish speaking television to start training my ears, but to do this I need a baseline of words I understand to be able to even know what they are saying!</p>
<p>Learning through apps like Duolingo, Drops, etc start with weird topics like vegetables that don't get you to a very good baseline for actually understanding daily conversations, so I think consuming TV is a better use of my time.</p>
<h2>Subtitles</h2>
<p>I've decided the way to understand what the best words to study are is to download every subtitle for every episode of a show I want to watch and then count each word.  The more a word is spoken the more important it is for me to know it since I'll be hearing it a lot in the show.</p>
<p>I'm going to download subtitles from Netflix. Subtitles in Netflix are in WebVTT format, which looks like this:</p>
<div data-rehype-pretty-code-fragment=""><pre class="github-dark-dimmed" style="background-color: #22272e" tabindex="0" data-language="" data-theme="default"><code data-language="" data-theme="default" style="display: grid;"><span data-line=""><span style="color: #adbac7">248</span></span>
<span data-line=""><span style="color: #adbac7">00:17:58.285 --> 00:18:01.163  position:50.00%,middle  align:middle size:80.00%  line:79.33% </span></span>
<span data-line=""><span style="color: #adbac7">Yo de verdad espero que ustedes</span></span>
<span data-line=""><span style="color: #adbac7">me vean como una amiga, ¿mmm?</span></span>
<span data-line=""><span style="color: #adbac7"></span></span>
<span data-line=""><span style="color: #adbac7">249</span></span>
<span data-line=""><span style="color: #adbac7">00:18:01.247 --> 00:18:02.539  position:50.00%,middle  align:middle size:80.00%  line:84.67% </span></span>
<span data-line=""><span style="color: #adbac7">No como una madrastra.</span></span>
<span data-line=""><span style="color: #adbac7"></span></span>
<span data-line=""><span style="color: #adbac7">250</span></span>
<span data-line=""><span style="color: #adbac7">00:18:04.250 --> 00:18:06.127  position:50.00%,middle  align:middle size:80.00%  line:84.67% </span></span>
<span data-line=""><span style="color: #adbac7">Yo nunca te vi como una madrastra.</span></span></code></pre></div>
<p>It gives you a start time, end time, and the text on the screen.   So my first process was parsing this format and just turning it into a list of words using https://github.com/glut23/webvtt-py.</p>
<h3>Dummy parsing</h3>
<p>What I basically did was <code>text.split(" ")</code> and started counting the words.   This approach was quick and painless but it had a few downs falls.    Some words <em>look</em> the same when in reality they are not and so this meant I'd have to study every meaning of a word even if it was more rare.</p>
<p>An example of this is the word "como", you can say:</p>
<ul>
<li>Haz como te digo: "Do as I say", where como means "as"</li>
<li>como tacos todos los dias: "I eat tacos every day", where como is a conjugated form of the verb "to eat"</li>
</ul>
<p>I need to know which version of a word is being used so I can count it properly.</p>
<h3>Regular Expressions are always the answer</h3>
<p>I couldn't figure out what the word was without it being in a complete sentence, but subtitles are fragments.   They are split up into timings for displaying on the screen but they don't include entire sentences.  For example, it might look like this:</p>
<div data-rehype-pretty-code-fragment=""><pre class="github-dark-dimmed" style="background-color: #22272e" tabindex="0" data-language="" data-theme="default"><code data-language="" data-theme="default" style="display: grid;"><span data-line=""><span style="color: #adbac7">23</span></span>
<span data-line=""><span style="color: #adbac7">00:01:21.960 --> 00:01:23.520  position:50.00%,middle  align:middle size:80.00%  line:84.67% </span></span>
<span data-line=""><span style="color: #adbac7">Solo las que luchan por ellos</span></span>
<span data-line=""><span style="color: #adbac7"></span></span>
<span data-line=""><span style="color: #adbac7">24</span></span>
<span data-line=""><span style="color: #adbac7">00:01:23.680 --> 00:01:25.680  position:50.00%,middle  align:middle size:80.00%  line:84.67% </span></span>
<span data-line=""><span style="color: #adbac7">consiguen sus sueños.</span></span></code></pre></div>
<p>I want to detect the start of a sentence and the end of a sentence and then combine it, so that you end up with "Solo las que luchan por ellos consiguen sus sueños.".   My first thought was a regular expression on punctuation.   This worked well <em>most</em> of the time but there were enough exceptions to the rule that it broke often on generated a lot of broken sentences:</p>
<ul>
<li>Abbreviations like "EE. UU" for estados unidos (united states)</li>
<li>Ellipsis</li>
</ul>
<p>Splitting on spaces also didn't work for identifying the parts of speech since I needed the context around the word.</p>
<center>
<img src="/images/posts/learning_spanish/regex-extraction.png">
</center>
<h2>Natural Language Processing</h2>
<p>So to solve my pain I decided to grab https://spacy.io/ and do some NLP on the subtitles so that I could identify the proper parts of speech and get an accurate representation of the words I needed to learn.</p>
<p>The way spaCy works is you can send it a sentence and it'll return you a set of tokens:</p>
<div data-rehype-pretty-code-fragment=""><pre class="github-dark-dimmed" style="background-color: #22272e" tabindex="0" data-language="" data-theme="default"><code data-language="" data-theme="default" style="display: grid;"><span data-line=""><span style="color: #adbac7">>>> import spacy</span></span>
<span data-line=""><span style="color: #adbac7">>>> nlp = spacy.load("es_core_news_sm")</span></span>
<span data-line=""><span style="color: #adbac7">>>> [x.pos_ for x in nlp("Hola, como estas?")]</span></span>
<span data-line=""><span style="color: #adbac7">['PROPN', 'PUNCT', 'SCONJ', 'PRON', 'PUNCT']</span></span></code></pre></div>
<p>So now I could identify the parts of speech and pull sentences together through end of sentence punctation.   The first thing I did was generate a CSV of sentences that looked like this:</p>
<table>
<tbody><tr>
<th>sentence</th>
<th>start</th>
<th>end</th>
<th>show</th>
<th>file</th>
</tr>
<tr>
<td>Si no, le voy a cortar todos los deditos</td>
<td>00:00:20.605</td>
<td>00:00:24.125</td>
<td>El marginal</td>
<td>El marginal S02E02 WEBRip Netflix es[cc].vtt</td>
</tr>
</tbody></table>
<p>Once I had a CSV of sentences I could send those back through spaCy for NLP and then start counting words, to generate another CSV:</p>
<table>
<tbody><tr>
<th>word</th>
<th>pos</th>
<th>show</th>
<th>file</th>
</tr>
<tr>
<td>a</td>
<td>ADP</td>
<td>El marginal</td>
<td>El marginal S02E02 WEBRip Netflix es[cc].vtt</td>
</tr>
<tr>
<td>cortar</td>
<td>VERB</td>
<td>El marginal</td>
<td>El marginal S02E02 WEBRip Netflix es[cc].vtt</td>
</tr>
<tr>
<td>todos</td>
<td>PRON</td>
<td>El marginal</td>
<td>El marginal S02E02 WEBRip Netflix es[cc].vtt</td>
</tr>
</tbody></table>
<p>From there I had all the data I needed!   So now it was time to start doing some data analysis!</p>
<h2>Data analysis</h2>
<p>Using a jupyter notebook ( https://jupyter.org/ ) I grabbed pandas ( https://pandas.pydata.org/ ) and read in my CSVs to start analyzing the results.</p>
<div data-rehype-pretty-code-fragment=""><pre class="github-dark-dimmed" style="background-color: #22272e" tabindex="0" data-language="" data-theme="default"><code data-language="" data-theme="default" style="display: grid;"><span data-line=""><span style="color: #adbac7">import numpy as np</span></span>
<span data-line=""><span style="color: #adbac7">import pandas as pd</span></span>
<span data-line=""><span style="color: #adbac7">import matplotlib.pyplot as plt</span></span>
<span data-line=""><span style="color: #adbac7">pd.set_option('display.max_rows', 1000)</span></span>
<span data-line=""><span style="color: #adbac7">words = pd.read_csv('word_data.csv.gz', compression='gzip', delimiter=',')</span></span></code></pre></div>
<p>The words dataframe is built up out of the second table I showed above with just words and their parts of speech.   I started off grouping the dataset by the word so I could get a count for how many times it was spoken in every series I parsed:</p>
<div data-rehype-pretty-code-fragment=""><pre class="github-dark-dimmed" style="background-color: #22272e" tabindex="0" data-language="" data-theme="default"><code data-language="" data-theme="default" style="display: grid;"><span data-line=""><span style="color: #adbac7">grouped_result = (words.groupby(words.word).size() </span></span>
<span data-line=""><span style="color: #adbac7">   .sort_values(ascending=False) </span></span>
<span data-line=""><span style="color: #adbac7">   .reset_index(name='count')</span></span>
<span data-line=""><span style="color: #adbac7">   .drop_duplicates(subset='word'))</span></span>
<span data-line=""><span style="color: #adbac7"></span></span>
<span data-line=""><span style="color: #adbac7">grouped_result.head(300)</span></span></code></pre></div>
<p>Which returned a list of words and their count:</p>
<div data-rehype-pretty-code-fragment=""><pre class="github-dark-dimmed" style="background-color: #22272e" tabindex="0" data-language="" data-theme="default"><code data-language="" data-theme="default" style="display: grid;"><span data-line=""><span style="color: #adbac7">	word	count</span></span>
<span data-line=""><span style="color: #adbac7">0	que	94430</span></span>
<span data-line=""><span style="color: #adbac7">1	no	75931</span></span>
<span data-line=""><span style="color: #adbac7">2	a	70968</span></span>
<span data-line=""><span style="color: #adbac7">3	de	67982</span></span>
<span data-line=""><span style="color: #adbac7">4	ser	64226</span></span>
<span data-line=""><span style="color: #adbac7">5	la	52143</span></span>
<span data-line=""><span style="color: #adbac7">6	y	44390</span></span>
<span data-line=""><span style="color: #adbac7">7	estar	37819</span></span>
<span data-line=""><span style="color: #adbac7">8	el	35920</span></span></code></pre></div>
<p>Now I wanted to identify where my diminishing returns would be.   Is there a set of words that I must learn because they are spoken so often that I wouldn't understand a conversation if they weren't in my vocabulary?</p>
<center>
<img src="/images/posts/learning_spanish/diminishing_returns.png">
</center>
<p>As you can see in this chart, the usage for words drops off at around the ~200 mark.   So there are basically 150 words I <em>must</em> know and then the rest are equally important.   I wasn't quite happy with this because some parts of speech are higher priority than others, for example I think having a strong understanding of the popular verbs will go a long way.  So I also wanted to identify what are the most important verbs to learn:</p>
<div data-rehype-pretty-code-fragment=""><pre class="github-dark-dimmed" style="background-color: #22272e" tabindex="0" data-language="" data-theme="default"><code data-language="" data-theme="default" style="display: grid;"><span data-line=""><span style="color: #adbac7">grouped_verbs = (words[words.pos == 'VERB'].groupby(['word', 'pos']).size() </span></span>
<span data-line=""><span style="color: #adbac7">   .sort_values(ascending=False) </span></span>
<span data-line=""><span style="color: #adbac7">   .reset_index(name='count')</span></span>
<span data-line=""><span style="color: #adbac7">   .drop_duplicates(subset='word'))</span></span>
<span data-line=""><span style="color: #adbac7"></span></span>
<span data-line=""><span style="color: #adbac7">grouped_verbs.head(50)</span></span></code></pre></div>
<p>Which got me this:</p>
<div data-rehype-pretty-code-fragment=""><pre class="github-dark-dimmed" style="background-color: #22272e" tabindex="0" data-language="" data-theme="default"><code data-language="" data-theme="default" style="display: grid;"><span data-line=""><span style="color: #adbac7">	word	pos	count</span></span>
<span data-line=""><span style="color: #adbac7">0	tener	VERB	22072</span></span>
<span data-line=""><span style="color: #adbac7">1	hacer	VERB	14946</span></span>
<span data-line=""><span style="color: #adbac7">2	ir	VERB	12570</span></span>
<span data-line=""><span style="color: #adbac7">3	decir	VERB	11314</span></span>
<span data-line=""><span style="color: #adbac7">4	querer	VERB	11083</span></span>
<span data-line=""><span style="color: #adbac7">5	ver	VERB	10269</span></span>
<span data-line=""><span style="color: #adbac7">6	estar	VERB	9780</span></span>
<span data-line=""><span style="color: #adbac7">7	saber	VERB	8704</span></span>
<span data-line=""><span style="color: #adbac7">8	ser	VERB	7674</span></span>
<span data-line=""><span style="color: #adbac7">9	dar	VERB	5722</span></span>
<span data-line=""><span style="color: #adbac7">10	pasar	VERB	5528</span></span>
<span data-line=""><span style="color: #adbac7">11	hablar	VERB	5355</span></span>
<span data-line=""><span style="color: #adbac7">12	venir	VERB	5145</span></span>
<span data-line=""><span style="color: #adbac7">13	creer	VERB	4895</span></span>
<span data-line=""><span style="color: #adbac7">14	salir 	VERB	3395</span></span></code></pre></div>
<p>Verbs had a slightly different drop-off pattern when I targeted them directly:</p>
<center>
<img src="/images/posts/learning_spanish/diminishing_verbs.png">
</center>
<p>I get a big bang for my buck by learning those top 40 verbs.   Nouns on the other hand are much more spread out and most are evenly distributed:</p>
<div data-rehype-pretty-code-fragment=""><pre class="github-dark-dimmed" style="background-color: #22272e" tabindex="0" data-language="" data-theme="default"><code data-language="" data-theme="default" style="display: grid;"><span data-line=""><span style="color: #adbac7">word	pos	count</span></span>
<span data-line=""><span style="color: #adbac7">0	gracias	NOUN	4676</span></span>
<span data-line=""><span style="color: #adbac7">1	favor	NOUN	4625</span></span>
<span data-line=""><span style="color: #adbac7">2	señor	NOUN	4116</span></span>
<span data-line=""><span style="color: #adbac7">3	verdad	NOUN	3566</span></span>
<span data-line=""><span style="color: #adbac7">4	vida	NOUN	2673</span></span>
<span data-line=""><span style="color: #adbac7">5	hombre	NOUN	2601</span></span>
<span data-line=""><span style="color: #adbac7">6	madre	NOUN	2597</span></span>
<span data-line=""><span style="color: #adbac7">7	vez	NOUN	2537</span></span>
<span data-line=""><span style="color: #adbac7">8	tiempo	NOUN	2492</span></span>
<span data-line=""><span style="color: #adbac7">9	hijo	NOUN	2215</span></span></code></pre></div>
<center>
<img src="/images/posts/learning_spanish/diminishing_nouns.png">
</center>
<p>So then I thought to myself... How much of a show would I understand if I just learned these most important words?  So I started by excluding some of the easy parts of speech and focused on the most important:</p>
<div data-rehype-pretty-code-fragment=""><pre class="github-dark-dimmed" style="background-color: #22272e" tabindex="0" data-language="" data-theme="default"><code data-language="" data-theme="default" style="display: grid;"><span data-line=""><span style="color: #adbac7">find_important_words = (words[~words.pos.isin(['PRON', 'CONJ', 'ADP', 'ADV', 'SCONJ', 'AUX', 'INTJ'])].groupby(['word', 'pos']).size() </span></span>
<span data-line=""><span style="color: #adbac7">   .sort_values(ascending=False) </span></span>
<span data-line=""><span style="color: #adbac7">   .reset_index(name='count')</span></span>
<span data-line=""><span style="color: #adbac7">   .drop_duplicates(subset='word'))</span></span>
<span data-line=""><span style="color: #adbac7"></span></span>
<span data-line=""><span style="color: #adbac7">find_important_words.head(50)</span></span></code></pre></div>
<p>The top 20 were all verbs except for <code>bueno</code> and <code>gracias</code>.   So now with my list of what I considered "important words" I plotted it to find what amount of words I wanted to learn:</p>
<center>
<img src="/images/posts/learning_spanish/important_words.png">
</center>
<p>It looks like 200 learned words would give me a reasonable amount of understanding for a series, so I decided to calculate how much of a series I would understand if I learned just those first 200 words:</p>
<div data-rehype-pretty-code-fragment=""><pre class="github-dark-dimmed" style="background-color: #22272e" tabindex="0" data-language="" data-theme="default"><code data-language="" data-theme="default" style="display: grid;"><span data-line=""><span style="color: #adbac7">percentages = {}</span></span>
<span data-line=""><span style="color: #adbac7"></span></span>
<span data-line=""><span style="color: #adbac7">for show_name in words['media'].drop_duplicates().values:</span></span>
<span data-line=""><span style="color: #adbac7">    words_in_show = (words[words.media == show_name].groupby(words.word).size() </span></span>
<span data-line=""><span style="color: #adbac7">       .sort_values(ascending=False) </span></span>
<span data-line=""><span style="color: #adbac7">       .reset_index(name='count')</span></span>
<span data-line=""><span style="color: #adbac7">       .drop_duplicates(subset='word'))</span></span>
<span data-line=""><span style="color: #adbac7">    </span></span>
<span data-line=""><span style="color: #adbac7">    total_words_handled = 0</span></span>
<span data-line=""><span style="color: #adbac7"></span></span>
<span data-line=""><span style="color: #adbac7">    for word in grouped_result['word'][:200]:</span></span>
<span data-line=""><span style="color: #adbac7">        values = words_in_show[words_in_show.word == word]['count'].values</span></span>
<span data-line=""><span style="color: #adbac7"></span></span>
<span data-line=""><span style="color: #adbac7">        if values.size > 0:</span></span>
<span data-line=""><span style="color: #adbac7">            total_words_handled += values[0]</span></span>
<span data-line=""><span style="color: #adbac7"></span></span>
<span data-line=""><span style="color: #adbac7">    percentages[show_name] = total_words_handled / words_in_show.sum().loc['count']</span></span></code></pre></div>
<p>Now I had a table that would show me what percentage of the spoken words were covered by the first 200 words in my list:</p>
<div data-rehype-pretty-code-fragment=""><pre class="github-dark-dimmed" style="background-color: #22272e" tabindex="0" data-language="" data-theme="default"><code data-language="" data-theme="default" style="display: grid;"><span data-line=""><span style="color: #adbac7">p_df = pd.DataFrame(percentages.items(), columns=['show', 'percentage'])</span></span>
<span data-line=""><span style="color: #adbac7">p_df = p_df.sort_values(by='percentage')</span></span>
<span data-line=""><span style="color: #adbac7">p_df['percentage'] = p_df['percentage'] * 100</span></span>
<span data-line=""><span style="color: #adbac7">pd.options.display.float_format = '{:,.2f}%'.format</span></span>
<span data-line=""><span style="color: #adbac7">p_df</span></span></code></pre></div>




















<table>
<tbody><tr>
<th>Show</th>
<th>Percentage</th>
</tr><tr>
<td>Verónica</td>
<td>64.24%</td>
</tr><tr>
<td>El ciudadano ilustre</td>
<td>65.28%</td>
</tr><tr>
<td>El Chapo</td>
<td>66.68%</td>
</tr><tr>
<td>Neruda</td>
<td>66.89%</td>
</tr><tr>
<td>La casa de papel</td>
<td>67.56%</td>
</tr><tr>
<td>El Ministerio del Tiempo</td>
<td>68.03%</td>
</tr><tr>
<td>Club de Cuervos</td>
<td>68.19%</td>
</tr><tr>
<td>El marginal</td>
<td>68.47%</td>
</tr><tr>
<td>Ingobernable</td>
<td>68.59%</td>
</tr><tr>
<td>Pablo Escobar</td>
<td>70.20%</td>
</tr><tr>
<td>Fariña</td>
<td>70.95</td>
</tr><tr>
<td>La Reina del Sur</td>
<td>71.52%</td>
</tr><tr>
<td>Gran Hotel</td>
<td>73.15%</td>
</tr><tr>
<td>Las chicas del cable</td>
<td>73.58%</td>
</tr><tr>
<td>Élite</td>
<td>73.78%</td>
</tr><tr>
<td>La Piloto</td>
<td>74.03%</td>
</tr><tr>
<td>El bar</td>
<td>74.07%</td>
</tr><tr>
<td>La casa de las flores</td>
<td>75.40%</td>
</tr><tr>
<td>Tarde para la ira</td>
<td>75.59%</td>
</tr></tbody></table>
<p>But living in Puerto Rico, one thing I've realized is speed of speech is also important.  I have a much easier time speaking with people from Colombia and Mexico than I do with Puerto Ricans because they speak so much faster.   So even though I could understand 75% of "Tarde para la ira" if I learned the 200 words, I want to make sure they are speaking at a pace I could understand as well.</p>
<p>So I loaded up the other CSV file that was the full sentences and added a "time per word" column:</p>
<div data-rehype-pretty-code-fragment=""><pre class="github-dark-dimmed" style="background-color: #22272e" tabindex="0" data-language="" data-theme="default"><code data-language="" data-theme="default" style="display: grid;"><span data-line=""><span style="color: #adbac7">sentences = pd.read_csv('sentences.csv.gz', compression='gzip', delimiter=',', parse_dates=['start', 'end'])</span></span>
<span data-line=""><span style="color: #adbac7">sentences['total_time'] = (sentences['end'] - sentences['start']).dt.total_seconds()</span></span>
<span data-line=""><span style="color: #adbac7">sentences['word_count'] = sentences['sentence'].str.split().str.len()</span></span>
<span data-line=""><span style="color: #adbac7">sentences['time_per_word'] = sentences['total_time'] / sentences['word_count']</span></span></code></pre></div>
<p>Then I was able to have a speed rating for each show:</p>
<div data-rehype-pretty-code-fragment=""><pre class="github-dark-dimmed" style="background-color: #22272e" tabindex="0" data-language="" data-theme="default"><code data-language="" data-theme="default" style="display: grid;"><span data-line=""><span style="color: #adbac7">sentence_group = sentences.groupby([sentences.media])</span></span>
<span data-line=""><span style="color: #adbac7">sentence_group.time_per_word.mean().reset_index().sort_values('time_per_word')</span></span></code></pre></div>




















<table>
<tbody><tr>
<th>media</th>
<th>time_per_word</th>
</tr><tr>
<td>Gran Hotel</td>
<td>0.58</td>
</tr><tr>
<td>El Chapo</td>
<td>0.59</td>
</tr><tr>
<td>Las chicas del cable</td>
<td>0.61</td>
</tr><tr>
<td>Élite</td>
<td>0.63</td>
</tr><tr>
<td>Ingobernable</td>
<td>0.64</td>
</tr><tr>
<td>El Ministerio del Tiempo</td>
<td>0.64</td>
</tr><tr>
<td>Fariña</td>
<td>0.65</td>
</tr><tr>
<td>El ciudadano ilustre</td>
<td>0.67</td>
</tr><tr>
<td>Neruda</td>
<td>0.68</td>
</tr><tr>
<td>La Piloto</td>
<td>0.69</td>
</tr><tr>
<td>La casa de papel</td>
<td>0.70</td>
</tr><tr>
<td>El bar</td>
<td>0.70</td>
</tr><tr>
<td>Verónica</td>
<td>0.72</td>
</tr><tr>
<td>La Reina del Sur</td>
<td>0.75</td>
</tr><tr>
<td>Club de Cuervos</td>
<td>0.76</td>
</tr><tr>
<td>El marginal</td>
<td>0.76</td>
</tr><tr>
<td>Pablo Escobar</td>
<td>0.77</td>
</tr><tr>
<td>Tarde para la ira</td>
<td>0.77</td>
</tr><tr>
<td>La casa de las flores</td>
<td>0.81</td>
</tr></tbody></table>
<p>Luckily the two series that have the least amount of vocabulary also speak the slowest!   So these will be the series I start with.    The final question I wanted to answer is "What are the top words I'm missing for a series".    Since I'll know 75% of the series from the top 200 words, I'm hoping there are some top words from a specific series that I can also learn to get an even higher understanding.</p>
<p>First, find which words are in each show but not in the top 200:</p>
<div data-rehype-pretty-code-fragment=""><pre class="github-dark-dimmed" style="background-color: #22272e" tabindex="0" data-language="" data-theme="default"><code data-language="" data-theme="default" style="display: grid;"><span data-line=""><span style="color: #adbac7">missing_words_by_show = {}</span></span>
<span data-line=""><span style="color: #adbac7"></span></span>
<span data-line=""><span style="color: #adbac7">for show_name in words['media'].drop_duplicates().values:</span></span>
<span data-line=""><span style="color: #adbac7">    words_in_show = (words[words.media == show_name].groupby(words.word).size() </span></span>
<span data-line=""><span style="color: #adbac7">       .sort_values(ascending=False) </span></span>
<span data-line=""><span style="color: #adbac7">       .reset_index(name='count')</span></span>
<span data-line=""><span style="color: #adbac7">       .drop_duplicates(subset='word'))</span></span>
<span data-line=""><span style="color: #adbac7">    </span></span>
<span data-line=""><span style="color: #adbac7">    frequency_words = grouped_result['word'][:200]</span></span>
<span data-line=""><span style="color: #adbac7"></span></span>
<span data-line=""><span style="color: #adbac7">    missing_words = words_in_show[~words_in_show.word.isin(frequency_words.values)]</span></span>
<span data-line=""><span style="color: #adbac7">    missing_words_by_show[show_name] = missing_words</span></span></code></pre></div>
<p>Then we were able to grab them per show:</p>
<div data-rehype-pretty-code-fragment=""><pre class="github-dark-dimmed" style="background-color: #22272e" tabindex="0" data-language="" data-theme="default"><code data-language="" data-theme="default" style="display: grid;"><span data-line=""><span style="color: #adbac7">missing_words_by_show['La casa de las flores'].head(50)</span></span>
<span data-line=""><span style="color: #adbac7"></span></span>
<span data-line=""><span style="color: #adbac7">word	count</span></span>
<span data-line=""><span style="color: #adbac7">31	mamá	252</span></span>
<span data-line=""><span style="color: #adbac7">70	florería	87</span></span>
<span data-line=""><span style="color: #adbac7">98	perdón	56</span></span>
<span data-line=""><span style="color: #adbac7">102	sea	54</span></span>
<span data-line=""><span style="color: #adbac7">116	además	44</span></span>
<span data-line=""><span style="color: #adbac7">126	ahorita	40</span></span>
<span data-line=""><span style="color: #adbac7">132	cárcel	38</span></span>
<span data-line=""><span style="color: #adbac7">133	fiesta	38</span></span></code></pre></div>
<p>So adding those few words to my vocabulary will also give me a better understanding of the series.</p>
<h2>Conclusion</h2>
<p>I believe a data-driven approach to language learning will be an effective way to get me speaking better spanish.   It was a ton of fun to play with spaCy, pandas, and jupyter as well!</p>
<p>I'll improve the data analysis over time as well but I do believe this is a pretty good starting point!</p>
<center>
<img src="/images/posts/learning_spanish/meme.png">
</center></div></article></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"postData":{"id":["2022","learning_spanish"],"path":"2022/learning_spanish","contentHtml":"\u003cp\u003eI've been living in Puerto Rico for 4 years but two of those have been COVID and so I haven't been able to practice Spanish as much as I'd like. So to speed up my learning I've decided I want to watch a lot of spanish speaking television to start training my ears, but to do this I need a baseline of words I understand to be able to even know what they are saying!\u003c/p\u003e\n\u003cp\u003eLearning through apps like Duolingo, Drops, etc start with weird topics like vegetables that don't get you to a very good baseline for actually understanding daily conversations, so I think consuming TV is a better use of my time.\u003c/p\u003e\n\u003ch2\u003eSubtitles\u003c/h2\u003e\n\u003cp\u003eI've decided the way to understand what the best words to study are is to download every subtitle for every episode of a show I want to watch and then count each word.  The more a word is spoken the more important it is for me to know it since I'll be hearing it a lot in the show.\u003c/p\u003e\n\u003cp\u003eI'm going to download subtitles from Netflix. Subtitles in Netflix are in WebVTT format, which looks like this:\u003c/p\u003e\n\u003cdiv data-rehype-pretty-code-fragment=\"\"\u003e\u003cpre class=\"github-dark-dimmed\" style=\"background-color: #22272e\" tabindex=\"0\" data-language=\"\" data-theme=\"default\"\u003e\u003ccode data-language=\"\" data-theme=\"default\" style=\"display: grid;\"\u003e\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e248\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e00:17:58.285 --\u003e 00:18:01.163  position:50.00%,middle  align:middle size:80.00%  line:79.33% \u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003eYo de verdad espero que ustedes\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003eme vean como una amiga, ¿mmm?\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e249\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e00:18:01.247 --\u003e 00:18:02.539  position:50.00%,middle  align:middle size:80.00%  line:84.67% \u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003eNo como una madrastra.\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e250\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e00:18:04.250 --\u003e 00:18:06.127  position:50.00%,middle  align:middle size:80.00%  line:84.67% \u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003eYo nunca te vi como una madrastra.\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eIt gives you a start time, end time, and the text on the screen.   So my first process was parsing this format and just turning it into a list of words using https://github.com/glut23/webvtt-py.\u003c/p\u003e\n\u003ch3\u003eDummy parsing\u003c/h3\u003e\n\u003cp\u003eWhat I basically did was \u003ccode\u003etext.split(\" \")\u003c/code\u003e and started counting the words.   This approach was quick and painless but it had a few downs falls.    Some words \u003cem\u003elook\u003c/em\u003e the same when in reality they are not and so this meant I'd have to study every meaning of a word even if it was more rare.\u003c/p\u003e\n\u003cp\u003eAn example of this is the word \"como\", you can say:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eHaz como te digo: \"Do as I say\", where como means \"as\"\u003c/li\u003e\n\u003cli\u003ecomo tacos todos los dias: \"I eat tacos every day\", where como is a conjugated form of the verb \"to eat\"\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI need to know which version of a word is being used so I can count it properly.\u003c/p\u003e\n\u003ch3\u003eRegular Expressions are always the answer\u003c/h3\u003e\n\u003cp\u003eI couldn't figure out what the word was without it being in a complete sentence, but subtitles are fragments.   They are split up into timings for displaying on the screen but they don't include entire sentences.  For example, it might look like this:\u003c/p\u003e\n\u003cdiv data-rehype-pretty-code-fragment=\"\"\u003e\u003cpre class=\"github-dark-dimmed\" style=\"background-color: #22272e\" tabindex=\"0\" data-language=\"\" data-theme=\"default\"\u003e\u003ccode data-language=\"\" data-theme=\"default\" style=\"display: grid;\"\u003e\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e23\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e00:01:21.960 --\u003e 00:01:23.520  position:50.00%,middle  align:middle size:80.00%  line:84.67% \u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003eSolo las que luchan por ellos\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e24\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e00:01:23.680 --\u003e 00:01:25.680  position:50.00%,middle  align:middle size:80.00%  line:84.67% \u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003econsiguen sus sueños.\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eI want to detect the start of a sentence and the end of a sentence and then combine it, so that you end up with \"Solo las que luchan por ellos consiguen sus sueños.\".   My first thought was a regular expression on punctuation.   This worked well \u003cem\u003emost\u003c/em\u003e of the time but there were enough exceptions to the rule that it broke often on generated a lot of broken sentences:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAbbreviations like \"EE. UU\" for estados unidos (united states)\u003c/li\u003e\n\u003cli\u003eEllipsis\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eSplitting on spaces also didn't work for identifying the parts of speech since I needed the context around the word.\u003c/p\u003e\n\u003ccenter\u003e\n\u003cimg src=\"/images/posts/learning_spanish/regex-extraction.png\"\u003e\n\u003c/center\u003e\n\u003ch2\u003eNatural Language Processing\u003c/h2\u003e\n\u003cp\u003eSo to solve my pain I decided to grab https://spacy.io/ and do some NLP on the subtitles so that I could identify the proper parts of speech and get an accurate representation of the words I needed to learn.\u003c/p\u003e\n\u003cp\u003eThe way spaCy works is you can send it a sentence and it'll return you a set of tokens:\u003c/p\u003e\n\u003cdiv data-rehype-pretty-code-fragment=\"\"\u003e\u003cpre class=\"github-dark-dimmed\" style=\"background-color: #22272e\" tabindex=\"0\" data-language=\"\" data-theme=\"default\"\u003e\u003ccode data-language=\"\" data-theme=\"default\" style=\"display: grid;\"\u003e\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e\u003e\u003e\u003e import spacy\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e\u003e\u003e\u003e nlp = spacy.load(\"es_core_news_sm\")\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e\u003e\u003e\u003e [x.pos_ for x in nlp(\"Hola, como estas?\")]\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e['PROPN', 'PUNCT', 'SCONJ', 'PRON', 'PUNCT']\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eSo now I could identify the parts of speech and pull sentences together through end of sentence punctation.   The first thing I did was generate a CSV of sentences that looked like this:\u003c/p\u003e\n\u003ctable\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003cth\u003esentence\u003c/th\u003e\n\u003cth\u003estart\u003c/th\u003e\n\u003cth\u003eend\u003c/th\u003e\n\u003cth\u003eshow\u003c/th\u003e\n\u003cth\u003efile\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eSi no, le voy a cortar todos los deditos\u003c/td\u003e\n\u003ctd\u003e00:00:20.605\u003c/td\u003e\n\u003ctd\u003e00:00:24.125\u003c/td\u003e\n\u003ctd\u003eEl marginal\u003c/td\u003e\n\u003ctd\u003eEl marginal S02E02 WEBRip Netflix es[cc].vtt\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003cp\u003eOnce I had a CSV of sentences I could send those back through spaCy for NLP and then start counting words, to generate another CSV:\u003c/p\u003e\n\u003ctable\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003cth\u003eword\u003c/th\u003e\n\u003cth\u003epos\u003c/th\u003e\n\u003cth\u003eshow\u003c/th\u003e\n\u003cth\u003efile\u003c/th\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003ea\u003c/td\u003e\n\u003ctd\u003eADP\u003c/td\u003e\n\u003ctd\u003eEl marginal\u003c/td\u003e\n\u003ctd\u003eEl marginal S02E02 WEBRip Netflix es[cc].vtt\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003ecortar\u003c/td\u003e\n\u003ctd\u003eVERB\u003c/td\u003e\n\u003ctd\u003eEl marginal\u003c/td\u003e\n\u003ctd\u003eEl marginal S02E02 WEBRip Netflix es[cc].vtt\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003etodos\u003c/td\u003e\n\u003ctd\u003ePRON\u003c/td\u003e\n\u003ctd\u003eEl marginal\u003c/td\u003e\n\u003ctd\u003eEl marginal S02E02 WEBRip Netflix es[cc].vtt\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003cp\u003eFrom there I had all the data I needed!   So now it was time to start doing some data analysis!\u003c/p\u003e\n\u003ch2\u003eData analysis\u003c/h2\u003e\n\u003cp\u003eUsing a jupyter notebook ( https://jupyter.org/ ) I grabbed pandas ( https://pandas.pydata.org/ ) and read in my CSVs to start analyzing the results.\u003c/p\u003e\n\u003cdiv data-rehype-pretty-code-fragment=\"\"\u003e\u003cpre class=\"github-dark-dimmed\" style=\"background-color: #22272e\" tabindex=\"0\" data-language=\"\" data-theme=\"default\"\u003e\u003ccode data-language=\"\" data-theme=\"default\" style=\"display: grid;\"\u003e\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003eimport numpy as np\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003eimport pandas as pd\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003eimport matplotlib.pyplot as plt\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003epd.set_option('display.max_rows', 1000)\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003ewords = pd.read_csv('word_data.csv.gz', compression='gzip', delimiter=',')\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThe words dataframe is built up out of the second table I showed above with just words and their parts of speech.   I started off grouping the dataset by the word so I could get a count for how many times it was spoken in every series I parsed:\u003c/p\u003e\n\u003cdiv data-rehype-pretty-code-fragment=\"\"\u003e\u003cpre class=\"github-dark-dimmed\" style=\"background-color: #22272e\" tabindex=\"0\" data-language=\"\" data-theme=\"default\"\u003e\u003ccode data-language=\"\" data-theme=\"default\" style=\"display: grid;\"\u003e\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003egrouped_result = (words.groupby(words.word).size() \u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e   .sort_values(ascending=False) \u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e   .reset_index(name='count')\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e   .drop_duplicates(subset='word'))\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003egrouped_result.head(300)\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eWhich returned a list of words and their count:\u003c/p\u003e\n\u003cdiv data-rehype-pretty-code-fragment=\"\"\u003e\u003cpre class=\"github-dark-dimmed\" style=\"background-color: #22272e\" tabindex=\"0\" data-language=\"\" data-theme=\"default\"\u003e\u003ccode data-language=\"\" data-theme=\"default\" style=\"display: grid;\"\u003e\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e\tword\tcount\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e0\tque\t94430\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e1\tno\t75931\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e2\ta\t70968\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e3\tde\t67982\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e4\tser\t64226\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e5\tla\t52143\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e6\ty\t44390\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e7\testar\t37819\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e8\tel\t35920\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eNow I wanted to identify where my diminishing returns would be.   Is there a set of words that I must learn because they are spoken so often that I wouldn't understand a conversation if they weren't in my vocabulary?\u003c/p\u003e\n\u003ccenter\u003e\n\u003cimg src=\"/images/posts/learning_spanish/diminishing_returns.png\"\u003e\n\u003c/center\u003e\n\u003cp\u003eAs you can see in this chart, the usage for words drops off at around the ~200 mark.   So there are basically 150 words I \u003cem\u003emust\u003c/em\u003e know and then the rest are equally important.   I wasn't quite happy with this because some parts of speech are higher priority than others, for example I think having a strong understanding of the popular verbs will go a long way.  So I also wanted to identify what are the most important verbs to learn:\u003c/p\u003e\n\u003cdiv data-rehype-pretty-code-fragment=\"\"\u003e\u003cpre class=\"github-dark-dimmed\" style=\"background-color: #22272e\" tabindex=\"0\" data-language=\"\" data-theme=\"default\"\u003e\u003ccode data-language=\"\" data-theme=\"default\" style=\"display: grid;\"\u003e\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003egrouped_verbs = (words[words.pos == 'VERB'].groupby(['word', 'pos']).size() \u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e   .sort_values(ascending=False) \u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e   .reset_index(name='count')\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e   .drop_duplicates(subset='word'))\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003egrouped_verbs.head(50)\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eWhich got me this:\u003c/p\u003e\n\u003cdiv data-rehype-pretty-code-fragment=\"\"\u003e\u003cpre class=\"github-dark-dimmed\" style=\"background-color: #22272e\" tabindex=\"0\" data-language=\"\" data-theme=\"default\"\u003e\u003ccode data-language=\"\" data-theme=\"default\" style=\"display: grid;\"\u003e\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e\tword\tpos\tcount\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e0\ttener\tVERB\t22072\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e1\thacer\tVERB\t14946\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e2\tir\tVERB\t12570\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e3\tdecir\tVERB\t11314\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e4\tquerer\tVERB\t11083\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e5\tver\tVERB\t10269\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e6\testar\tVERB\t9780\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e7\tsaber\tVERB\t8704\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e8\tser\tVERB\t7674\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e9\tdar\tVERB\t5722\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e10\tpasar\tVERB\t5528\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e11\thablar\tVERB\t5355\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e12\tvenir\tVERB\t5145\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e13\tcreer\tVERB\t4895\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e14\tsalir \tVERB\t3395\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eVerbs had a slightly different drop-off pattern when I targeted them directly:\u003c/p\u003e\n\u003ccenter\u003e\n\u003cimg src=\"/images/posts/learning_spanish/diminishing_verbs.png\"\u003e\n\u003c/center\u003e\n\u003cp\u003eI get a big bang for my buck by learning those top 40 verbs.   Nouns on the other hand are much more spread out and most are evenly distributed:\u003c/p\u003e\n\u003cdiv data-rehype-pretty-code-fragment=\"\"\u003e\u003cpre class=\"github-dark-dimmed\" style=\"background-color: #22272e\" tabindex=\"0\" data-language=\"\" data-theme=\"default\"\u003e\u003ccode data-language=\"\" data-theme=\"default\" style=\"display: grid;\"\u003e\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003eword\tpos\tcount\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e0\tgracias\tNOUN\t4676\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e1\tfavor\tNOUN\t4625\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e2\tseñor\tNOUN\t4116\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e3\tverdad\tNOUN\t3566\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e4\tvida\tNOUN\t2673\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e5\thombre\tNOUN\t2601\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e6\tmadre\tNOUN\t2597\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e7\tvez\tNOUN\t2537\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e8\ttiempo\tNOUN\t2492\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e9\thijo\tNOUN\t2215\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003ccenter\u003e\n\u003cimg src=\"/images/posts/learning_spanish/diminishing_nouns.png\"\u003e\n\u003c/center\u003e\n\u003cp\u003eSo then I thought to myself... How much of a show would I understand if I just learned these most important words?  So I started by excluding some of the easy parts of speech and focused on the most important:\u003c/p\u003e\n\u003cdiv data-rehype-pretty-code-fragment=\"\"\u003e\u003cpre class=\"github-dark-dimmed\" style=\"background-color: #22272e\" tabindex=\"0\" data-language=\"\" data-theme=\"default\"\u003e\u003ccode data-language=\"\" data-theme=\"default\" style=\"display: grid;\"\u003e\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003efind_important_words = (words[~words.pos.isin(['PRON', 'CONJ', 'ADP', 'ADV', 'SCONJ', 'AUX', 'INTJ'])].groupby(['word', 'pos']).size() \u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e   .sort_values(ascending=False) \u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e   .reset_index(name='count')\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e   .drop_duplicates(subset='word'))\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003efind_important_words.head(50)\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThe top 20 were all verbs except for \u003ccode\u003ebueno\u003c/code\u003e and \u003ccode\u003egracias\u003c/code\u003e.   So now with my list of what I considered \"important words\" I plotted it to find what amount of words I wanted to learn:\u003c/p\u003e\n\u003ccenter\u003e\n\u003cimg src=\"/images/posts/learning_spanish/important_words.png\"\u003e\n\u003c/center\u003e\n\u003cp\u003eIt looks like 200 learned words would give me a reasonable amount of understanding for a series, so I decided to calculate how much of a series I would understand if I learned just those first 200 words:\u003c/p\u003e\n\u003cdiv data-rehype-pretty-code-fragment=\"\"\u003e\u003cpre class=\"github-dark-dimmed\" style=\"background-color: #22272e\" tabindex=\"0\" data-language=\"\" data-theme=\"default\"\u003e\u003ccode data-language=\"\" data-theme=\"default\" style=\"display: grid;\"\u003e\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003epercentages = {}\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003efor show_name in words['media'].drop_duplicates().values:\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e    words_in_show = (words[words.media == show_name].groupby(words.word).size() \u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e       .sort_values(ascending=False) \u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e       .reset_index(name='count')\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e       .drop_duplicates(subset='word'))\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e    \u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e    total_words_handled = 0\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e    for word in grouped_result['word'][:200]:\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e        values = words_in_show[words_in_show.word == word]['count'].values\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e        if values.size \u003e 0:\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e            total_words_handled += values[0]\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e    percentages[show_name] = total_words_handled / words_in_show.sum().loc['count']\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eNow I had a table that would show me what percentage of the spoken words were covered by the first 200 words in my list:\u003c/p\u003e\n\u003cdiv data-rehype-pretty-code-fragment=\"\"\u003e\u003cpre class=\"github-dark-dimmed\" style=\"background-color: #22272e\" tabindex=\"0\" data-language=\"\" data-theme=\"default\"\u003e\u003ccode data-language=\"\" data-theme=\"default\" style=\"display: grid;\"\u003e\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003ep_df = pd.DataFrame(percentages.items(), columns=['show', 'percentage'])\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003ep_df = p_df.sort_values(by='percentage')\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003ep_df['percentage'] = p_df['percentage'] * 100\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003epd.options.display.float_format = '{:,.2f}%'.format\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003ep_df\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003ctable\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003cth\u003eShow\u003c/th\u003e\n\u003cth\u003ePercentage\u003c/th\u003e\n\u003c/tr\u003e\u003ctr\u003e\n\u003ctd\u003eVerónica\u003c/td\u003e\n\u003ctd\u003e64.24%\u003c/td\u003e\n\u003c/tr\u003e\u003ctr\u003e\n\u003ctd\u003eEl ciudadano ilustre\u003c/td\u003e\n\u003ctd\u003e65.28%\u003c/td\u003e\n\u003c/tr\u003e\u003ctr\u003e\n\u003ctd\u003eEl Chapo\u003c/td\u003e\n\u003ctd\u003e66.68%\u003c/td\u003e\n\u003c/tr\u003e\u003ctr\u003e\n\u003ctd\u003eNeruda\u003c/td\u003e\n\u003ctd\u003e66.89%\u003c/td\u003e\n\u003c/tr\u003e\u003ctr\u003e\n\u003ctd\u003eLa casa de papel\u003c/td\u003e\n\u003ctd\u003e67.56%\u003c/td\u003e\n\u003c/tr\u003e\u003ctr\u003e\n\u003ctd\u003eEl Ministerio del Tiempo\u003c/td\u003e\n\u003ctd\u003e68.03%\u003c/td\u003e\n\u003c/tr\u003e\u003ctr\u003e\n\u003ctd\u003eClub de Cuervos\u003c/td\u003e\n\u003ctd\u003e68.19%\u003c/td\u003e\n\u003c/tr\u003e\u003ctr\u003e\n\u003ctd\u003eEl marginal\u003c/td\u003e\n\u003ctd\u003e68.47%\u003c/td\u003e\n\u003c/tr\u003e\u003ctr\u003e\n\u003ctd\u003eIngobernable\u003c/td\u003e\n\u003ctd\u003e68.59%\u003c/td\u003e\n\u003c/tr\u003e\u003ctr\u003e\n\u003ctd\u003ePablo Escobar\u003c/td\u003e\n\u003ctd\u003e70.20%\u003c/td\u003e\n\u003c/tr\u003e\u003ctr\u003e\n\u003ctd\u003eFariña\u003c/td\u003e\n\u003ctd\u003e70.95\u003c/td\u003e\n\u003c/tr\u003e\u003ctr\u003e\n\u003ctd\u003eLa Reina del Sur\u003c/td\u003e\n\u003ctd\u003e71.52%\u003c/td\u003e\n\u003c/tr\u003e\u003ctr\u003e\n\u003ctd\u003eGran Hotel\u003c/td\u003e\n\u003ctd\u003e73.15%\u003c/td\u003e\n\u003c/tr\u003e\u003ctr\u003e\n\u003ctd\u003eLas chicas del cable\u003c/td\u003e\n\u003ctd\u003e73.58%\u003c/td\u003e\n\u003c/tr\u003e\u003ctr\u003e\n\u003ctd\u003eÉlite\u003c/td\u003e\n\u003ctd\u003e73.78%\u003c/td\u003e\n\u003c/tr\u003e\u003ctr\u003e\n\u003ctd\u003eLa Piloto\u003c/td\u003e\n\u003ctd\u003e74.03%\u003c/td\u003e\n\u003c/tr\u003e\u003ctr\u003e\n\u003ctd\u003eEl bar\u003c/td\u003e\n\u003ctd\u003e74.07%\u003c/td\u003e\n\u003c/tr\u003e\u003ctr\u003e\n\u003ctd\u003eLa casa de las flores\u003c/td\u003e\n\u003ctd\u003e75.40%\u003c/td\u003e\n\u003c/tr\u003e\u003ctr\u003e\n\u003ctd\u003eTarde para la ira\u003c/td\u003e\n\u003ctd\u003e75.59%\u003c/td\u003e\n\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\n\u003cp\u003eBut living in Puerto Rico, one thing I've realized is speed of speech is also important.  I have a much easier time speaking with people from Colombia and Mexico than I do with Puerto Ricans because they speak so much faster.   So even though I could understand 75% of \"Tarde para la ira\" if I learned the 200 words, I want to make sure they are speaking at a pace I could understand as well.\u003c/p\u003e\n\u003cp\u003eSo I loaded up the other CSV file that was the full sentences and added a \"time per word\" column:\u003c/p\u003e\n\u003cdiv data-rehype-pretty-code-fragment=\"\"\u003e\u003cpre class=\"github-dark-dimmed\" style=\"background-color: #22272e\" tabindex=\"0\" data-language=\"\" data-theme=\"default\"\u003e\u003ccode data-language=\"\" data-theme=\"default\" style=\"display: grid;\"\u003e\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003esentences = pd.read_csv('sentences.csv.gz', compression='gzip', delimiter=',', parse_dates=['start', 'end'])\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003esentences['total_time'] = (sentences['end'] - sentences['start']).dt.total_seconds()\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003esentences['word_count'] = sentences['sentence'].str.split().str.len()\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003esentences['time_per_word'] = sentences['total_time'] / sentences['word_count']\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThen I was able to have a speed rating for each show:\u003c/p\u003e\n\u003cdiv data-rehype-pretty-code-fragment=\"\"\u003e\u003cpre class=\"github-dark-dimmed\" style=\"background-color: #22272e\" tabindex=\"0\" data-language=\"\" data-theme=\"default\"\u003e\u003ccode data-language=\"\" data-theme=\"default\" style=\"display: grid;\"\u003e\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003esentence_group = sentences.groupby([sentences.media])\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003esentence_group.time_per_word.mean().reset_index().sort_values('time_per_word')\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003ctable\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003cth\u003emedia\u003c/th\u003e\n\u003cth\u003etime_per_word\u003c/th\u003e\n\u003c/tr\u003e\u003ctr\u003e\n\u003ctd\u003eGran Hotel\u003c/td\u003e\n\u003ctd\u003e0.58\u003c/td\u003e\n\u003c/tr\u003e\u003ctr\u003e\n\u003ctd\u003eEl Chapo\u003c/td\u003e\n\u003ctd\u003e0.59\u003c/td\u003e\n\u003c/tr\u003e\u003ctr\u003e\n\u003ctd\u003eLas chicas del cable\u003c/td\u003e\n\u003ctd\u003e0.61\u003c/td\u003e\n\u003c/tr\u003e\u003ctr\u003e\n\u003ctd\u003eÉlite\u003c/td\u003e\n\u003ctd\u003e0.63\u003c/td\u003e\n\u003c/tr\u003e\u003ctr\u003e\n\u003ctd\u003eIngobernable\u003c/td\u003e\n\u003ctd\u003e0.64\u003c/td\u003e\n\u003c/tr\u003e\u003ctr\u003e\n\u003ctd\u003eEl Ministerio del Tiempo\u003c/td\u003e\n\u003ctd\u003e0.64\u003c/td\u003e\n\u003c/tr\u003e\u003ctr\u003e\n\u003ctd\u003eFariña\u003c/td\u003e\n\u003ctd\u003e0.65\u003c/td\u003e\n\u003c/tr\u003e\u003ctr\u003e\n\u003ctd\u003eEl ciudadano ilustre\u003c/td\u003e\n\u003ctd\u003e0.67\u003c/td\u003e\n\u003c/tr\u003e\u003ctr\u003e\n\u003ctd\u003eNeruda\u003c/td\u003e\n\u003ctd\u003e0.68\u003c/td\u003e\n\u003c/tr\u003e\u003ctr\u003e\n\u003ctd\u003eLa Piloto\u003c/td\u003e\n\u003ctd\u003e0.69\u003c/td\u003e\n\u003c/tr\u003e\u003ctr\u003e\n\u003ctd\u003eLa casa de papel\u003c/td\u003e\n\u003ctd\u003e0.70\u003c/td\u003e\n\u003c/tr\u003e\u003ctr\u003e\n\u003ctd\u003eEl bar\u003c/td\u003e\n\u003ctd\u003e0.70\u003c/td\u003e\n\u003c/tr\u003e\u003ctr\u003e\n\u003ctd\u003eVerónica\u003c/td\u003e\n\u003ctd\u003e0.72\u003c/td\u003e\n\u003c/tr\u003e\u003ctr\u003e\n\u003ctd\u003eLa Reina del Sur\u003c/td\u003e\n\u003ctd\u003e0.75\u003c/td\u003e\n\u003c/tr\u003e\u003ctr\u003e\n\u003ctd\u003eClub de Cuervos\u003c/td\u003e\n\u003ctd\u003e0.76\u003c/td\u003e\n\u003c/tr\u003e\u003ctr\u003e\n\u003ctd\u003eEl marginal\u003c/td\u003e\n\u003ctd\u003e0.76\u003c/td\u003e\n\u003c/tr\u003e\u003ctr\u003e\n\u003ctd\u003ePablo Escobar\u003c/td\u003e\n\u003ctd\u003e0.77\u003c/td\u003e\n\u003c/tr\u003e\u003ctr\u003e\n\u003ctd\u003eTarde para la ira\u003c/td\u003e\n\u003ctd\u003e0.77\u003c/td\u003e\n\u003c/tr\u003e\u003ctr\u003e\n\u003ctd\u003eLa casa de las flores\u003c/td\u003e\n\u003ctd\u003e0.81\u003c/td\u003e\n\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\n\u003cp\u003eLuckily the two series that have the least amount of vocabulary also speak the slowest!   So these will be the series I start with.    The final question I wanted to answer is \"What are the top words I'm missing for a series\".    Since I'll know 75% of the series from the top 200 words, I'm hoping there are some top words from a specific series that I can also learn to get an even higher understanding.\u003c/p\u003e\n\u003cp\u003eFirst, find which words are in each show but not in the top 200:\u003c/p\u003e\n\u003cdiv data-rehype-pretty-code-fragment=\"\"\u003e\u003cpre class=\"github-dark-dimmed\" style=\"background-color: #22272e\" tabindex=\"0\" data-language=\"\" data-theme=\"default\"\u003e\u003ccode data-language=\"\" data-theme=\"default\" style=\"display: grid;\"\u003e\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003emissing_words_by_show = {}\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003efor show_name in words['media'].drop_duplicates().values:\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e    words_in_show = (words[words.media == show_name].groupby(words.word).size() \u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e       .sort_values(ascending=False) \u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e       .reset_index(name='count')\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e       .drop_duplicates(subset='word'))\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e    \u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e    frequency_words = grouped_result['word'][:200]\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e    missing_words = words_in_show[~words_in_show.word.isin(frequency_words.values)]\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e    missing_words_by_show[show_name] = missing_words\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThen we were able to grab them per show:\u003c/p\u003e\n\u003cdiv data-rehype-pretty-code-fragment=\"\"\u003e\u003cpre class=\"github-dark-dimmed\" style=\"background-color: #22272e\" tabindex=\"0\" data-language=\"\" data-theme=\"default\"\u003e\u003ccode data-language=\"\" data-theme=\"default\" style=\"display: grid;\"\u003e\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003emissing_words_by_show['La casa de las flores'].head(50)\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003eword\tcount\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e31\tmamá\t252\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e70\tflorería\t87\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e98\tperdón\t56\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e102\tsea\t54\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e116\tademás\t44\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e126\tahorita\t40\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e132\tcárcel\t38\u003c/span\u003e\u003c/span\u003e\n\u003cspan data-line=\"\"\u003e\u003cspan style=\"color: #adbac7\"\u003e133\tfiesta\t38\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eSo adding those few words to my vocabulary will also give me a better understanding of the series.\u003c/p\u003e\n\u003ch2\u003eConclusion\u003c/h2\u003e\n\u003cp\u003eI believe a data-driven approach to language learning will be an effective way to get me speaking better spanish.   It was a ton of fun to play with spaCy, pandas, and jupyter as well!\u003c/p\u003e\n\u003cp\u003eI'll improve the data analysis over time as well but I do believe this is a pretty good starting point!\u003c/p\u003e\n\u003ccenter\u003e\n\u003cimg src=\"/images/posts/learning_spanish/meme.png\"\u003e\n\u003c/center\u003e","category":"Development","date":"2022-04-30T00:00:00Z","tags":["Python","Pandas","NLP"],"title":"How to speak spanish like a colombian drug lord!"}},"__N_SSG":true},"page":"/blog/[...id]","query":{"id":["2022","learning_spanish"]},"buildId":"AoGbC8xlMEkyyIwWs3twS","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>