<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><link rel="alternate" type="application/rss+xml" title="sontek&#x27;s Engineering Blog" href="/rss.xml"/><title>Running a kubernetes cluster locally with kubeadm</title><meta name="next-head-count" content="4"/><link rel="preload" href="/_next/static/css/59351a1314c8a8b6.css" as="style"/><link rel="stylesheet" href="/_next/static/css/59351a1314c8a8b6.css" data-n-g=""/><link rel="preload" href="/_next/static/css/95e79f6f6f52c645.css" as="style"/><link rel="stylesheet" href="/_next/static/css/95e79f6f6f52c645.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-d7b038a63b619762.js" defer=""></script><script src="/_next/static/chunks/framework-4556c45dd113b893.js" defer=""></script><script src="/_next/static/chunks/main-2a9c662ddd7329fb.js" defer=""></script><script src="/_next/static/chunks/pages/_app-267db8380a174e21.js" defer=""></script><script src="/_next/static/chunks/996-9e3c12b77542c098.js" defer=""></script><script src="/_next/static/chunks/771-5e4a02ed896d5599.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5B...id%5D-426e76fedd45ec38.js" defer=""></script><script src="/_next/static/uPL-WT8At1JvJM6WotaJL/_buildManifest.js" defer=""></script><script src="/_next/static/uPL-WT8At1JvJM6WotaJL/_ssgManifest.js" defer=""></script><script src="/_next/static/uPL-WT8At1JvJM6WotaJL/_middlewareManifest.js" defer=""></script></head><body><div id="__next"><div><div class="grid"><div class="col"><header id="banner" class="body"><h1><a href="/">sontek.net</a></h1></header></div><div class="col menu"><nav><ul><li><a href="/">Home</a></li><li><a href="/blog">Blog</a></li><li><a href="/resume">Resume</a></li><li><a href="/about">About</a></li></ul></nav></div></div><div class="container"><article class="blog_article__NoEy4"><h1 class="util_headingXl__knZ_h">Running a kubernetes cluster locally with kubeadm</h1><div class="util_lightText__xcqUE"><time dateTime="2022-04-16T20:00:00-04:00">April 16, 2022</time></div><div><p>I’m going to show you how to get a real kubernetes cluster setup locally on top of virtual
machines!  I’ll be using multipass but feel free to use virtualbox, proxmox, or whatever your
favorite cloud provider is.</p>
<p>kubeadm a production ready kubernetes install tool and I prefer to use it over minikube, kind,
etc. because it gives you a more real world experience for <em>managing</em> the kubernetes cluster.
This isn’t important if you are a user of the cluster but if you have to run your own this is
a great way to gain some daily experience.</p>
<p>The kubernetes documentation on kubeadm is great and you can find it <a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/">here</a>.</p>
<p>The differences between this blog and the kubernetes docs is that they leave a lot of decisions
up to the reader such as:</p>
<ul>
<li>choosing a container runtime</li>
<li>Selecting and installing a CNI (container network interface)</li>
</ul>
<p>I’m going to be opinionated and make specific technology decisions such as using containerd and
cilium so that you don't have to think about those decisions.</p>
<h2>Getting your Virtual Machines setup!</h2>
<p>The minimum requirements for a control plane node in kubernetes is 2gb of RAM and 2 CPUs.  Since
we actually want to be able to schedule workloads on the workers afterwards we are going to setup
a cluster that looks like this:</p>
<ul>
<li>Control Plane: 2gb RAM, 2 CPU</li>
<li>Worker: 4gb RAM, 2 CPU</li>
</ul>
<p>Since we’ll be using multipass to launch the nodes, we can do that now:</p>
<pre><code class="hljs language-bash">❯ multipass launch -c 2 -m 4G -d 10G -n controlplane 22.04
❯ multipass launch -c 2 -m 4G -d 10G -n worker 22.04
❯ multipass list
Name                    State             IPv4             Image
controlplane            Running           192.168.64.7     Ubuntu 22.04 LTS
worker                  Running           192.168.64.8     Ubuntu 22.04 LTS
</code></pre>
<p>Now we can start working on our controlplane first, lets shell in:</p>
<pre><code class="hljs language-bash">❯ multipass shell controlplane
</code></pre>
<p>Lets first add the kubernetes repo to the system so we have access to all the kubernetes tools:</p>
<pre><code class="hljs language-bash">❯ <span class="hljs-built_in">echo</span> <span class="hljs-string">"deb  http://apt.kubernetes.io/  kubernetes-xenial  main"</span> | sudo <span class="hljs-built_in">tee</span> /etc/apt/sources.list.d/kubernetes.list

❯ curl -fsSL  https://packages.cloud.google.com/apt/doc/apt-key.gpg|sudo gpg --dearmor -o /etc/apt/trusted.gpg.d/k8s.gpg
❯ sudo apt-get update &#x26;&#x26; sudo apt-get upgrade -y
</code></pre>
<p>Now that our system is setup, we can move on to getting a container runtime.</p>
<h2>Getting your Container Runtime!</h2>
<p>Before we start pulling in kubernetes components we need to get a container runtime setup on the
machine.   We we are going to use containerd for this purpose.  You can view the docs of for it
<a href="https://github.com/containerd/containerd/blob/main/docs/getting-started.md">here</a>.</p>
<p>Which will download the latest binary and set it up.   I’m going to walk you through how to do it
using the version packaged with Ubuntu which could be older than the latest release.</p>
<p>First thing we want to do is configure the networking to allow iptables to manage:</p>
<pre><code class="hljs language-bash">❯ <span class="hljs-built_in">cat</span> &#x3C;&#x3C;<span class="hljs-string">EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF</span>

❯ <span class="hljs-built_in">cat</span> &#x3C;&#x3C;<span class="hljs-string">EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
EOF</span>

</code></pre>
<p>We also need to disable some default systemd settings for <code>rp_filter</code>  because
they are not compatible with cilium. See the bug report
<a href="https://github.com/cilium/cilium/commit/cabc6581b8128681f4ed23f8d6dc463180eea61e">here</a></p>
<pre><code class="hljs language-bash">❯ sudo sed -i -e <span class="hljs-string">'/net.ipv4.conf.*.rp_filter/d'</span> $(grep -ril <span class="hljs-string">'\.rp_filter'</span> /etc/sysctl.d/ /usr/lib/sysctl.d/)
❯ sudo sysctl -a | grep <span class="hljs-string">'\.rp_filter'</span> | awk <span class="hljs-string">'{print $1" = 0"}'</span> | sudo <span class="hljs-built_in">tee</span> -a /etc/sysctl.d/1000-cilium.conf
</code></pre>
<p>Then we need to refresh sysctl so those settings are applied:</p>
<pre><code class="hljs language-bash">❯ sudo systemctl restart systemd-modules-load
❯ sudo sysctl --system
</code></pre>
<p>You should see it applying all the changes:</p>
<pre><code class="hljs language-ini">* Applying /etc/sysctl.d/k8s.conf ...
<span class="hljs-attr">net.bridge.bridge-nf-call-ip6tables</span> = <span class="hljs-number">1</span>
<span class="hljs-attr">net.bridge.bridge-nf-call-iptables</span> = <span class="hljs-number">1</span>
<span class="hljs-attr">net.ipv4.ip_forward</span> = <span class="hljs-number">1</span>
</code></pre>
<p>If you do not, the netfilter module may not have loaded properly:</p>
<pre><code class="hljs language-bash">❯ lsmod |grep br_netfilter
br_netfilter           28672  0
bridge                176128  1 br_netfilter
</code></pre>
<p>You want to make sure <code>rp_filter</code> is <code>0</code> everywhere as well for cilium:</p>
<pre><code class="hljs language-ini">❯ sudo sysctl -a | grep '\.rp_filter'
<span class="hljs-attr">net.ipv4.conf.all.rp_filter</span> = <span class="hljs-number">0</span>
<span class="hljs-attr">net.ipv4.conf.cilium_host.rp_filter</span> = <span class="hljs-number">0</span>
<span class="hljs-attr">net.ipv4.conf.cilium_net.rp_filter</span> = <span class="hljs-number">0</span>
<span class="hljs-attr">net.ipv4.conf.cilium_vxlan.rp_filter</span> = <span class="hljs-number">0</span>
<span class="hljs-attr">net.ipv4.conf.default.rp_filter</span> = <span class="hljs-number">0</span>
<span class="hljs-attr">net.ipv4.conf.enp0s1.rp_filter</span> = <span class="hljs-number">0</span>
<span class="hljs-attr">net.ipv4.conf.lo.rp_filter</span> = <span class="hljs-number">0</span>
<span class="hljs-attr">net.ipv4.conf.lxc0965b7b545f7.rp_filter</span> = <span class="hljs-number">0</span>
<span class="hljs-attr">net.ipv4.conf.lxcb05ffd84ab74.rp_filter</span> = <span class="hljs-number">0</span>
</code></pre>
<p>Now lets pull down the container runtime we’ll be using which is containerd.</p>
<p>Ubuntu ships with a very old version of containerd so you need to upgrade to
the version shipped from the docker repos:
You can find which versions are available by running:</p>
<pre><code class="hljs language-bash">❯ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/trusted.gpg.d/docker.gpg
❯ <span class="hljs-built_in">echo</span> <span class="hljs-string">"deb https://download.docker.com/linux/ubuntu <span class="hljs-subst">$(lsb_release -cs)</span> stable"</span> | sudo <span class="hljs-built_in">tee</span> /etc/apt/sources.list.d/docker.list
❯ sudo apt-get update
</code></pre>
<pre><code class="hljs language-bash">❯ sudo apt-cache madison containerd.io
containerd.io |    1.6.8-1 | https://download.docker.com/linux/ubuntu jammy/stable arm64 Packages
containerd.io |    1.6.7-1 | https://download.docker.com/linux/ubuntu jammy/stable arm64 Packages
containerd.io |    1.6.6-1 | https://download.docker.com/linux/ubuntu jammy/stable arm64 Packages
containerd.io |    1.6.4-1 | https://download.docker.com/linux/ubuntu jammy/stable arm64 Packages
containerd.io |   1.5.11-1 | https://download.docker.com/linux/ubuntu jammy/stable arm64 Packages
containerd.io |   1.5.10-1 | https://download.docker.com/linux/ubuntu jammy/stable arm64 Packages
</code></pre>
<p>We are going to use the latest version available which was 1.6.8-1</p>
<pre><code class="hljs language-bash">❯ sudo apt-get install containerd.io=1.6.8-1 -y
</code></pre>
<p>Then we'll setup a configuration that enables containerd to use the systemd
cgroup.  We are hard coding this config instead of using <code>containerd config default</code>
because that currently has had a <a href="https://github.com/containerd/containerd/issues/4574">bug</a>
for many years that generates an invalid config.</p>
<pre><code class="hljs language-bash">❯ <span class="hljs-built_in">cat</span> &#x3C;&#x3C;<span class="hljs-string">EOF | sudo tee /etc/containerd/config.toml
version = 2
[plugins]
  [plugins."io.containerd.grpc.v1.cri"]
   [plugins."io.containerd.grpc.v1.cri".containerd]
      [plugins."io.containerd.grpc.v1.cri".containerd.runtimes]
        [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
          runtime_type = "io.containerd.runc.v2"
          [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
            SystemdCgroup = true
EOF</span>

❯ sudo systemctl restart containerd.service
</code></pre>
<p>You can verify its running with ctr:</p>
<pre><code class="hljs language-bash">❯ sudo ctr --address /var/run/containerd/containerd.sock containers list
CONTAINER    IMAGE    RUNTIME
</code></pre>
<p>Now that this is working we can move on to getting kubernetes installed!</p>
<h2>Using kubeadm!</h2>
<p>Now we need to get the kubernetes tools installed onto the system.  I’m going to be using 1.23
but to find the latest version you can run:</p>
<pre><code class="hljs language-bash">❯ sudo apt-cache madison kubeadm|<span class="hljs-built_in">head</span> -n2
   kubeadm |  1.23.5-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
   kubeadm |  1.23.4-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
</code></pre>
<p>Then install the version you want, we install kubelet and kubeadm here to make
sure the versions align:</p>
<pre><code class="hljs language-bash">❯ sudo apt-get install kubeadm=1.23.5-00 kubelet=1.23.5-00 kubectl=1.23.5-00 -y
</code></pre>
<p>This will pull in a few tools, including an alternative to <code>ctr</code> that we used earlier called
<code>crictl</code>.  You can check that it is available to you doing this:</p>
<pre><code class="hljs language-bash">❯ sudo crictl --runtime-endpoint=unix:///var/run/containerd/containerd.sock ps
</code></pre>
<p>We can finally init our cluster:</p>
<pre><code class="hljs language-bash">❯ sudo kubeadm init
</code></pre>
<p>Once that finishes running it should give you some tips setup your configuration, it should look like this:</p>
<pre><code class="hljs language-bash">❯ <span class="hljs-built_in">mkdir</span> -p <span class="hljs-variable">$HOME</span>/.kube
❯ sudo <span class="hljs-built_in">cp</span> -i /etc/kubernetes/admin.conf <span class="hljs-variable">$HOME</span>/.kube/config
❯ sudo <span class="hljs-built_in">chown</span> $(<span class="hljs-built_in">id</span> -u):$(<span class="hljs-built_in">id</span> -g) <span class="hljs-variable">$HOME</span>/.kube/config
</code></pre>
<p>You can run those on the master node for now, but later I'll show you how to move
the config to your host computer.</p>
<p>Now you should be able to check that your node is not ready yet:</p>
<pre><code class="hljs language-bash">❯ kubectl get nodes
NAME           STATUS     ROLES                  AGE     VERSION
controlplane   NotReady   control-plane,master   4m16s   v1.23.5
</code></pre>
<p><em>Note</em>: If you recieve "The connecto to the server was refused" error,
The cluster starting up and getting all the dependencies running could take
a bit of time.  So if you aren't able to communicate right away you can check
which pods are up and running with <code>crictl</code>.  You'll need <code>kube-apiserver</code> up
and running.  If it isn't you can check:</p>
<pre><code class="hljs language-bash">❯ sudo crictl --runtime-endpoint=unix:///var/run/containerd/containerd.sock ps -a
CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
8322192c4605c       bd8cc6d582470       36 seconds ago      Running             kube-proxy                4                   344c4f7fffbe8       kube-proxy-drm46
30ce27c40adb2       81a4a8a4ac639       2 minutes ago       Exited              kube-controller-manager   4                   3a819c3a864b2       kube-controller-manager-controlplane
7709fd5e92898       bd8cc6d582470       2 minutes ago       Exited              kube-proxy                3                   7cc6922c82015       kube-proxy-drm46
10432b81d7c61       3767741e7fba7       2 minutes ago       Exited              kube-apiserver            4                   e64ddf3679d98       kube-apiserver-controlplane
</code></pre>
<p>which will show you pods that have exited. You can grab the container ID for
kube-apiserver and read its logs:</p>
<pre><code class="hljs language-bash">❯ sudo crictl --runtime-endpoint=unix:///var/run/containerd/containerd.sock logs 10432b81d7c61
</code></pre>
<p>There are a few ways to figure out why the node isn’t ready yet.  Usually I would check the
<code>kubelet</code> logs first:</p>
<pre><code class="hljs language-bash">❯ sudo journalctl -flu kubelet
-- Logs begin at Sun 2022-04-17 19:22:19 AST. --
Apr 17 20:53:15 controlplane kubelet[19727]: E0417 20:53:15.951350   19727 kubelet.go:2347] <span class="hljs-string">"Container runtime network not ready"</span> networkReady=<span class="hljs-string">"NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized"</span>
Apr 17 20:53:20 controlplane kubelet[19727]: E0417 20:53:20.952148   19727 kubelet.go:2347] <span class="hljs-string">"Container runtime network not ready"</span> networkReady=<span class="hljs-string">"NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized"</span>
</code></pre>
<p>It is clear the problem is that we are missing the CNI.  The other way you can find out what is
going on is describing the node:</p>
<pre><code class="hljs language-bash">❯ kubectl describe node controlplane
</code></pre>
<p>This will have a lot of information but if you scroll through there looking at <code>Reason</code> you
might see something useful.  In this case under <code>Lease</code> you would see:</p>
<pre><code class="hljs language-bash">❯ kubectl describe node controlplane|grep NotReady
Ready            False   Sun, 17 Apr 2022 20:53:37 -0400   Sun, 17 Apr 2022 20:43:07 -0400   KubeletNotReady              container runtime network not ready: NetworkReady=<span class="hljs-literal">false</span> reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialize
</code></pre>
<p>Lets get our CNI installed, we’ll be using cilium!</p>
<h2>Setting up your CNI!</h2>
<p>Cilium has great documentation over <a href="https://docs.cilium.io/en/v1.9/gettingstarted/k8s-install-kubeadm/">here</a>,
but I’ll walk you through it anyways.  I do recommend checking out their documentation so you
are familiar with it.   We will use <code>helm</code> to pull down the version of cilium we want:</p>
<pre><code class="hljs language-bash">❯ curl -fsSL  https://baltocdn.com/helm/signing.asc | sudo gpg --dearmor -o /etc/apt/trusted.gpg.d/helm.gpg

❯ sudo apt-get install apt-transport-https --<span class="hljs-built_in">yes</span>

❯ <span class="hljs-built_in">echo</span> <span class="hljs-string">"deb https://baltocdn.com/helm/stable/debian/ all main"</span> | sudo <span class="hljs-built_in">tee</span> /etc/apt/sources.list.d/helm-stable-debian.list

❯ sudo apt-get update
❯ sudo apt-get install helm
</code></pre>
<p>Now we can install cilium!  It is <em>very</em> important that you pay attention to the
compatibility of cilium with the version of kubernetes you are intstalling. Check
the compatibility list <a href="https://docs.cilium.io/en/v1.12/concepts/kubernetes/compatibility/">here</a>.</p>
<pre><code class="hljs language-bash">❯ helm repo add cilium https://helm.cilium.io/
❯ helm repo update
</code></pre>
<p>Once the repo is added you can list the versions available:</p>
<pre><code class="hljs language-bash">❯ helm search repo -l|<span class="hljs-built_in">head</span> -n8
NAME           	CHART VERSION	APP VERSION	DESCRIPTION
cilium/cilium  	1.12.1       	1.12.1     	eBPF-based Networking, Security, and Observability
cilium/cilium  	1.12.0       	1.12.0     	eBPF-based Networking, Security, and Observability
cilium/cilium  	1.11.8       	1.11.8     	eBPF-based Networking, Security, and Observability
cilium/cilium  	1.11.7       	1.11.7     	eBPF-based Networking, Security, and Observability
cilium/cilium  	1.11.6       	1.11.6     	eBPF-based Networking, Security, and Observability
cilium/cilium  	1.11.5       	1.11.5     	eBPF-based Networking, Security, and Observability
cilium/cilium  	1.11.4       	1.11.4     	eBPF-based Networking, Security, and Observability
</code></pre>
<p>So we want <code>1.11.4</code>:</p>
<pre><code class="hljs language-bash">❯ helm install cilium cilium/cilium --namespace kube-system --version 1.11.4
</code></pre>
<p>Now our node should be ready!</p>
<pre><code class="hljs language-bash">❯ kubectl get node
NAME           STATUS   ROLES                  AGE   VERSION
controlplane   Ready    control-plane,master   24m   v1.23.5
</code></pre>
<p>Time to join our worker to the cluster!</p>
<h2>Joining a worker to the cluster!</h2>
<p>We have to go through the same steps as the controlplane to get the point that we have a
container runtime and <code>kubeadm</code>.   I’m not going to talk about the commands a second time but
I’ll re-iterate them here for ease of following along.</p>
<p>First open up another shell and connect to the worker:</p>
<pre><code class="hljs language-bash">❯ multipass shell worker
</code></pre>
<p>Now run the following commands:</p>
<pre><code class="hljs language-bash">❯ <span class="hljs-built_in">echo</span> <span class="hljs-string">"deb  http://apt.kubernetes.io/  kubernetes-xenial  main"</span> | sudo <span class="hljs-built_in">tee</span> /etc/apt/sources.list.d/kubernetes.list
❯ curl -fsSL  https://packages.cloud.google.com/apt/doc/apt-key.gpg|sudo gpg --dearmor -o /etc/apt/trusted.gpg.d/k8s.gpg
❯ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/trusted.gpg.d/docker.gpg
❯ <span class="hljs-built_in">echo</span> <span class="hljs-string">"deb https://download.docker.com/linux/ubuntu <span class="hljs-subst">$(lsb_release -cs)</span> stable"</span> | sudo <span class="hljs-built_in">tee</span> /etc/apt/sources.list.d/docker.list

❯ <span class="hljs-built_in">cat</span> &#x3C;&#x3C;<span class="hljs-string">EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF</span>

❯ sudo sed -i -e <span class="hljs-string">'/net.ipv4.conf.*.rp_filter/d'</span> $(grep -ril <span class="hljs-string">'\.rp_filter'</span> /etc/sysctl.d/ /usr/lib/sysctl.d/)
❯ sudo sysctl -a | grep <span class="hljs-string">'\.rp_filter'</span> | awk <span class="hljs-string">'{print $1" = 0"}'</span> | sudo <span class="hljs-built_in">tee</span> -a /etc/sysctl.d/1000-cilium.conf

❯ <span class="hljs-built_in">cat</span> &#x3C;&#x3C;<span class="hljs-string">EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
EOF</span>

❯ sudo systemctl restart systemd-modules-load
❯ sudo sysctl --system

❯ sudo apt-get update &#x26;&#x26; sudo apt-get upgrade -y
❯ sudo apt-get install containerd.io=1.6.8-1 -y

❯ <span class="hljs-built_in">cat</span> &#x3C;&#x3C;<span class="hljs-string">EOF | sudo tee /etc/containerd/config.toml
version = 2
[plugins]
  [plugins."io.containerd.grpc.v1.cri"]
   [plugins."io.containerd.grpc.v1.cri".containerd]
      [plugins."io.containerd.grpc.v1.cri".containerd.runtimes]
        [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
          runtime_type = "io.containerd.runc.v2"
          [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
            SystemdCgroup = true
EOF</span>

❯ sudo systemctl restart containerd.service
❯ sudo apt-get install kubeadm=1.23.5-00 kubelet=1.23.5-00 kubectl=1.23.5-00 -y

</code></pre>
<p>From there we should be ready to join the cluster.   When we ran <code>kubeadm init</code> previously it
printed a join command out that we could use but I’m going to show you how to do it if you
were coming back later and no longer had that token.</p>
<p>Back on the <em>controplane</em> node run:</p>
<pre><code class="hljs language-bash">❯ kubeadm token create --print-join-command
kubeadm <span class="hljs-built_in">join</span> 192.168.64.7:6443 --token wxs197.cco6mjj9ricvu8ov --discovery-token-ca-cert-hash sha256:bd01c065240fa76f30a02ecb70a8cea6e329c9678994d4da1f6ccac7694b97fb
</code></pre>
<p>Now copy that command and run it with <code>sudo</code> on the worker:</p>
<pre><code class="hljs language-bash">❯ sudo kubeadm <span class="hljs-built_in">join</span> 192.168.64.7:6443 --token wxs197.cco6mjj9ricvu8ov --discovery-token-ca-cert-hash sha256:bd01c065240fa76f30a02ecb70a8cea6e329c9678994d4da1f6ccac7694b97fb
</code></pre>
<p>After this completes it’ll take a minute or two for everything to be synced up but if you go
back to the master node you should have 2 ready nodes now:</p>
<pre><code class="hljs language-bash">❯ kubectl get nodes
NAME           STATUS   ROLES                  AGE   VERSION
controlplane   Ready    control-plane,master   46m   v1.23.5
worker         Ready    &#x3C;none>                 79s   v1.23.5
</code></pre>
<h2>Accessing the cluster outside of the VMs!</h2>
<p>Now the final part is to get the <code>admin.conf</code> as a kubeconfig on your machine so you can control
it from outside of the cluster.   To do this we can use scp</p>
<pre><code class="hljs language-bash">multipass transfer controlplane:/home/ubuntu/.kube/config local.config
</code></pre>
<p>Normally kubernetes configuration is in ~/.kube/config but I like to maint a separate file for
each cluster and then I set the <code>KUBECONFIG</code> env var to access it.</p>
<pre><code class="hljs language-bash">❯ <span class="hljs-built_in">export</span> KUBECONFIG=local.config
❯ kubectl get nodes
NAME           STATUS   ROLES                  AGE   VERSION
controlplane   Ready    control-plane,master   56m   v1.23.5
worker         Ready    &#x3C;none>                 11m   v1.23.5
</code></pre></div></article></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"postData":{"id":["2022","local_kubeadm_cluster"],"path":"2022/local_kubeadm_cluster","contentHtml":"\u003cp\u003eI’m going to show you how to get a real kubernetes cluster setup locally on top of virtual\nmachines!  I’ll be using multipass but feel free to use virtualbox, proxmox, or whatever your\nfavorite cloud provider is.\u003c/p\u003e\n\u003cp\u003ekubeadm a production ready kubernetes install tool and I prefer to use it over minikube, kind,\netc. because it gives you a more real world experience for \u003cem\u003emanaging\u003c/em\u003e the kubernetes cluster.\nThis isn’t important if you are a user of the cluster but if you have to run your own this is\na great way to gain some daily experience.\u003c/p\u003e\n\u003cp\u003eThe kubernetes documentation on kubeadm is great and you can find it \u003ca href=\"https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThe differences between this blog and the kubernetes docs is that they leave a lot of decisions\nup to the reader such as:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003echoosing a container runtime\u003c/li\u003e\n\u003cli\u003eSelecting and installing a CNI (container network interface)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI’m going to be opinionated and make specific technology decisions such as using containerd and\ncilium so that you don't have to think about those decisions.\u003c/p\u003e\n\u003ch2\u003eGetting your Virtual Machines setup!\u003c/h2\u003e\n\u003cp\u003eThe minimum requirements for a control plane node in kubernetes is 2gb of RAM and 2 CPUs.  Since\nwe actually want to be able to schedule workloads on the workers afterwards we are going to setup\na cluster that looks like this:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eControl Plane: 2gb RAM, 2 CPU\u003c/li\u003e\n\u003cli\u003eWorker: 4gb RAM, 2 CPU\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eSince we’ll be using multipass to launch the nodes, we can do that now:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e❯ multipass launch -c 2 -m 4G -d 10G -n controlplane 22.04\n❯ multipass launch -c 2 -m 4G -d 10G -n worker 22.04\n❯ multipass list\nName                    State             IPv4             Image\ncontrolplane            Running           192.168.64.7     Ubuntu 22.04 LTS\nworker                  Running           192.168.64.8     Ubuntu 22.04 LTS\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNow we can start working on our controlplane first, lets shell in:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e❯ multipass shell controlplane\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eLets first add the kubernetes repo to the system so we have access to all the kubernetes tools:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e❯ \u003cspan class=\"hljs-built_in\"\u003eecho\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e\"deb  http://apt.kubernetes.io/  kubernetes-xenial  main\"\u003c/span\u003e | sudo \u003cspan class=\"hljs-built_in\"\u003etee\u003c/span\u003e /etc/apt/sources.list.d/kubernetes.list\n\n❯ curl -fsSL  https://packages.cloud.google.com/apt/doc/apt-key.gpg|sudo gpg --dearmor -o /etc/apt/trusted.gpg.d/k8s.gpg\n❯ sudo apt-get update \u0026#x26;\u0026#x26; sudo apt-get upgrade -y\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNow that our system is setup, we can move on to getting a container runtime.\u003c/p\u003e\n\u003ch2\u003eGetting your Container Runtime!\u003c/h2\u003e\n\u003cp\u003eBefore we start pulling in kubernetes components we need to get a container runtime setup on the\nmachine.   We we are going to use containerd for this purpose.  You can view the docs of for it\n\u003ca href=\"https://github.com/containerd/containerd/blob/main/docs/getting-started.md\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eWhich will download the latest binary and set it up.   I’m going to walk you through how to do it\nusing the version packaged with Ubuntu which could be older than the latest release.\u003c/p\u003e\n\u003cp\u003eFirst thing we want to do is configure the networking to allow iptables to manage:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e❯ \u003cspan class=\"hljs-built_in\"\u003ecat\u003c/span\u003e \u0026#x3C;\u0026#x3C;\u003cspan class=\"hljs-string\"\u003eEOF | sudo tee /etc/modules-load.d/k8s.conf\noverlay\nbr_netfilter\nEOF\u003c/span\u003e\n\n❯ \u003cspan class=\"hljs-built_in\"\u003ecat\u003c/span\u003e \u0026#x3C;\u0026#x3C;\u003cspan class=\"hljs-string\"\u003eEOF | sudo tee /etc/sysctl.d/k8s.conf\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables  = 1\nnet.ipv4.ip_forward                 = 1\nEOF\u003c/span\u003e\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWe also need to disable some default systemd settings for \u003ccode\u003erp_filter\u003c/code\u003e  because\nthey are not compatible with cilium. See the bug report\n\u003ca href=\"https://github.com/cilium/cilium/commit/cabc6581b8128681f4ed23f8d6dc463180eea61e\"\u003ehere\u003c/a\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e❯ sudo sed -i -e \u003cspan class=\"hljs-string\"\u003e'/net.ipv4.conf.*.rp_filter/d'\u003c/span\u003e $(grep -ril \u003cspan class=\"hljs-string\"\u003e'\\.rp_filter'\u003c/span\u003e /etc/sysctl.d/ /usr/lib/sysctl.d/)\n❯ sudo sysctl -a | grep \u003cspan class=\"hljs-string\"\u003e'\\.rp_filter'\u003c/span\u003e | awk \u003cspan class=\"hljs-string\"\u003e'{print $1\" = 0\"}'\u003c/span\u003e | sudo \u003cspan class=\"hljs-built_in\"\u003etee\u003c/span\u003e -a /etc/sysctl.d/1000-cilium.conf\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThen we need to refresh sysctl so those settings are applied:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e❯ sudo systemctl restart systemd-modules-load\n❯ sudo sysctl --system\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou should see it applying all the changes:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-ini\"\u003e* Applying /etc/sysctl.d/k8s.conf ...\n\u003cspan class=\"hljs-attr\"\u003enet.bridge.bridge-nf-call-ip6tables\u003c/span\u003e = \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e\n\u003cspan class=\"hljs-attr\"\u003enet.bridge.bridge-nf-call-iptables\u003c/span\u003e = \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e\n\u003cspan class=\"hljs-attr\"\u003enet.ipv4.ip_forward\u003c/span\u003e = \u003cspan class=\"hljs-number\"\u003e1\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIf you do not, the netfilter module may not have loaded properly:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e❯ lsmod |grep br_netfilter\nbr_netfilter           28672  0\nbridge                176128  1 br_netfilter\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou want to make sure \u003ccode\u003erp_filter\u003c/code\u003e is \u003ccode\u003e0\u003c/code\u003e everywhere as well for cilium:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-ini\"\u003e❯ sudo sysctl -a | grep '\\.rp_filter'\n\u003cspan class=\"hljs-attr\"\u003enet.ipv4.conf.all.rp_filter\u003c/span\u003e = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e\n\u003cspan class=\"hljs-attr\"\u003enet.ipv4.conf.cilium_host.rp_filter\u003c/span\u003e = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e\n\u003cspan class=\"hljs-attr\"\u003enet.ipv4.conf.cilium_net.rp_filter\u003c/span\u003e = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e\n\u003cspan class=\"hljs-attr\"\u003enet.ipv4.conf.cilium_vxlan.rp_filter\u003c/span\u003e = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e\n\u003cspan class=\"hljs-attr\"\u003enet.ipv4.conf.default.rp_filter\u003c/span\u003e = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e\n\u003cspan class=\"hljs-attr\"\u003enet.ipv4.conf.enp0s1.rp_filter\u003c/span\u003e = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e\n\u003cspan class=\"hljs-attr\"\u003enet.ipv4.conf.lo.rp_filter\u003c/span\u003e = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e\n\u003cspan class=\"hljs-attr\"\u003enet.ipv4.conf.lxc0965b7b545f7.rp_filter\u003c/span\u003e = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e\n\u003cspan class=\"hljs-attr\"\u003enet.ipv4.conf.lxcb05ffd84ab74.rp_filter\u003c/span\u003e = \u003cspan class=\"hljs-number\"\u003e0\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNow lets pull down the container runtime we’ll be using which is containerd.\u003c/p\u003e\n\u003cp\u003eUbuntu ships with a very old version of containerd so you need to upgrade to\nthe version shipped from the docker repos:\nYou can find which versions are available by running:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e❯ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/trusted.gpg.d/docker.gpg\n❯ \u003cspan class=\"hljs-built_in\"\u003eecho\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e\"deb https://download.docker.com/linux/ubuntu \u003cspan class=\"hljs-subst\"\u003e$(lsb_release -cs)\u003c/span\u003e stable\"\u003c/span\u003e | sudo \u003cspan class=\"hljs-built_in\"\u003etee\u003c/span\u003e /etc/apt/sources.list.d/docker.list\n❯ sudo apt-get update\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e❯ sudo apt-cache madison containerd.io\ncontainerd.io |    1.6.8-1 | https://download.docker.com/linux/ubuntu jammy/stable arm64 Packages\ncontainerd.io |    1.6.7-1 | https://download.docker.com/linux/ubuntu jammy/stable arm64 Packages\ncontainerd.io |    1.6.6-1 | https://download.docker.com/linux/ubuntu jammy/stable arm64 Packages\ncontainerd.io |    1.6.4-1 | https://download.docker.com/linux/ubuntu jammy/stable arm64 Packages\ncontainerd.io |   1.5.11-1 | https://download.docker.com/linux/ubuntu jammy/stable arm64 Packages\ncontainerd.io |   1.5.10-1 | https://download.docker.com/linux/ubuntu jammy/stable arm64 Packages\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWe are going to use the latest version available which was 1.6.8-1\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e❯ sudo apt-get install containerd.io=1.6.8-1 -y\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThen we'll setup a configuration that enables containerd to use the systemd\ncgroup.  We are hard coding this config instead of using \u003ccode\u003econtainerd config default\u003c/code\u003e\nbecause that currently has had a \u003ca href=\"https://github.com/containerd/containerd/issues/4574\"\u003ebug\u003c/a\u003e\nfor many years that generates an invalid config.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e❯ \u003cspan class=\"hljs-built_in\"\u003ecat\u003c/span\u003e \u0026#x3C;\u0026#x3C;\u003cspan class=\"hljs-string\"\u003eEOF | sudo tee /etc/containerd/config.toml\nversion = 2\n[plugins]\n  [plugins.\"io.containerd.grpc.v1.cri\"]\n   [plugins.\"io.containerd.grpc.v1.cri\".containerd]\n      [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes]\n        [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc]\n          runtime_type = \"io.containerd.runc.v2\"\n          [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc.options]\n            SystemdCgroup = true\nEOF\u003c/span\u003e\n\n❯ sudo systemctl restart containerd.service\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou can verify its running with ctr:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e❯ sudo ctr --address /var/run/containerd/containerd.sock containers list\nCONTAINER    IMAGE    RUNTIME\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNow that this is working we can move on to getting kubernetes installed!\u003c/p\u003e\n\u003ch2\u003eUsing kubeadm!\u003c/h2\u003e\n\u003cp\u003eNow we need to get the kubernetes tools installed onto the system.  I’m going to be using 1.23\nbut to find the latest version you can run:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e❯ sudo apt-cache madison kubeadm|\u003cspan class=\"hljs-built_in\"\u003ehead\u003c/span\u003e -n2\n   kubeadm |  1.23.5-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages\n   kubeadm |  1.23.4-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThen install the version you want, we install kubelet and kubeadm here to make\nsure the versions align:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e❯ sudo apt-get install kubeadm=1.23.5-00 kubelet=1.23.5-00 kubectl=1.23.5-00 -y\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis will pull in a few tools, including an alternative to \u003ccode\u003ectr\u003c/code\u003e that we used earlier called\n\u003ccode\u003ecrictl\u003c/code\u003e.  You can check that it is available to you doing this:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e❯ sudo crictl --runtime-endpoint=unix:///var/run/containerd/containerd.sock ps\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWe can finally init our cluster:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e❯ sudo kubeadm init\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOnce that finishes running it should give you some tips setup your configuration, it should look like this:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e❯ \u003cspan class=\"hljs-built_in\"\u003emkdir\u003c/span\u003e -p \u003cspan class=\"hljs-variable\"\u003e$HOME\u003c/span\u003e/.kube\n❯ sudo \u003cspan class=\"hljs-built_in\"\u003ecp\u003c/span\u003e -i /etc/kubernetes/admin.conf \u003cspan class=\"hljs-variable\"\u003e$HOME\u003c/span\u003e/.kube/config\n❯ sudo \u003cspan class=\"hljs-built_in\"\u003echown\u003c/span\u003e $(\u003cspan class=\"hljs-built_in\"\u003eid\u003c/span\u003e -u):$(\u003cspan class=\"hljs-built_in\"\u003eid\u003c/span\u003e -g) \u003cspan class=\"hljs-variable\"\u003e$HOME\u003c/span\u003e/.kube/config\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou can run those on the master node for now, but later I'll show you how to move\nthe config to your host computer.\u003c/p\u003e\n\u003cp\u003eNow you should be able to check that your node is not ready yet:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e❯ kubectl get nodes\nNAME           STATUS     ROLES                  AGE     VERSION\ncontrolplane   NotReady   control-plane,master   4m16s   v1.23.5\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cem\u003eNote\u003c/em\u003e: If you recieve \"The connecto to the server was refused\" error,\nThe cluster starting up and getting all the dependencies running could take\na bit of time.  So if you aren't able to communicate right away you can check\nwhich pods are up and running with \u003ccode\u003ecrictl\u003c/code\u003e.  You'll need \u003ccode\u003ekube-apiserver\u003c/code\u003e up\nand running.  If it isn't you can check:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e❯ sudo crictl --runtime-endpoint=unix:///var/run/containerd/containerd.sock ps -a\nCONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD\n8322192c4605c       bd8cc6d582470       36 seconds ago      Running             kube-proxy                4                   344c4f7fffbe8       kube-proxy-drm46\n30ce27c40adb2       81a4a8a4ac639       2 minutes ago       Exited              kube-controller-manager   4                   3a819c3a864b2       kube-controller-manager-controlplane\n7709fd5e92898       bd8cc6d582470       2 minutes ago       Exited              kube-proxy                3                   7cc6922c82015       kube-proxy-drm46\n10432b81d7c61       3767741e7fba7       2 minutes ago       Exited              kube-apiserver            4                   e64ddf3679d98       kube-apiserver-controlplane\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ewhich will show you pods that have exited. You can grab the container ID for\nkube-apiserver and read its logs:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e❯ sudo crictl --runtime-endpoint=unix:///var/run/containerd/containerd.sock logs 10432b81d7c61\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThere are a few ways to figure out why the node isn’t ready yet.  Usually I would check the\n\u003ccode\u003ekubelet\u003c/code\u003e logs first:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e❯ sudo journalctl -flu kubelet\n-- Logs begin at Sun 2022-04-17 19:22:19 AST. --\nApr 17 20:53:15 controlplane kubelet[19727]: E0417 20:53:15.951350   19727 kubelet.go:2347] \u003cspan class=\"hljs-string\"\u003e\"Container runtime network not ready\"\u003c/span\u003e networkReady=\u003cspan class=\"hljs-string\"\u003e\"NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized\"\u003c/span\u003e\nApr 17 20:53:20 controlplane kubelet[19727]: E0417 20:53:20.952148   19727 kubelet.go:2347] \u003cspan class=\"hljs-string\"\u003e\"Container runtime network not ready\"\u003c/span\u003e networkReady=\u003cspan class=\"hljs-string\"\u003e\"NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized\"\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIt is clear the problem is that we are missing the CNI.  The other way you can find out what is\ngoing on is describing the node:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e❯ kubectl describe node controlplane\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis will have a lot of information but if you scroll through there looking at \u003ccode\u003eReason\u003c/code\u003e you\nmight see something useful.  In this case under \u003ccode\u003eLease\u003c/code\u003e you would see:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e❯ kubectl describe node controlplane|grep NotReady\nReady            False   Sun, 17 Apr 2022 20:53:37 -0400   Sun, 17 Apr 2022 20:43:07 -0400   KubeletNotReady              container runtime network not ready: NetworkReady=\u003cspan class=\"hljs-literal\"\u003efalse\u003c/span\u003e reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialize\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eLets get our CNI installed, we’ll be using cilium!\u003c/p\u003e\n\u003ch2\u003eSetting up your CNI!\u003c/h2\u003e\n\u003cp\u003eCilium has great documentation over \u003ca href=\"https://docs.cilium.io/en/v1.9/gettingstarted/k8s-install-kubeadm/\"\u003ehere\u003c/a\u003e,\nbut I’ll walk you through it anyways.  I do recommend checking out their documentation so you\nare familiar with it.   We will use \u003ccode\u003ehelm\u003c/code\u003e to pull down the version of cilium we want:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e❯ curl -fsSL  https://baltocdn.com/helm/signing.asc | sudo gpg --dearmor -o /etc/apt/trusted.gpg.d/helm.gpg\n\n❯ sudo apt-get install apt-transport-https --\u003cspan class=\"hljs-built_in\"\u003eyes\u003c/span\u003e\n\n❯ \u003cspan class=\"hljs-built_in\"\u003eecho\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e\"deb https://baltocdn.com/helm/stable/debian/ all main\"\u003c/span\u003e | sudo \u003cspan class=\"hljs-built_in\"\u003etee\u003c/span\u003e /etc/apt/sources.list.d/helm-stable-debian.list\n\n❯ sudo apt-get update\n❯ sudo apt-get install helm\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNow we can install cilium!  It is \u003cem\u003every\u003c/em\u003e important that you pay attention to the\ncompatibility of cilium with the version of kubernetes you are intstalling. Check\nthe compatibility list \u003ca href=\"https://docs.cilium.io/en/v1.12/concepts/kubernetes/compatibility/\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e❯ helm repo add cilium https://helm.cilium.io/\n❯ helm repo update\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOnce the repo is added you can list the versions available:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e❯ helm search repo -l|\u003cspan class=\"hljs-built_in\"\u003ehead\u003c/span\u003e -n8\nNAME           \tCHART VERSION\tAPP VERSION\tDESCRIPTION\ncilium/cilium  \t1.12.1       \t1.12.1     \teBPF-based Networking, Security, and Observability\ncilium/cilium  \t1.12.0       \t1.12.0     \teBPF-based Networking, Security, and Observability\ncilium/cilium  \t1.11.8       \t1.11.8     \teBPF-based Networking, Security, and Observability\ncilium/cilium  \t1.11.7       \t1.11.7     \teBPF-based Networking, Security, and Observability\ncilium/cilium  \t1.11.6       \t1.11.6     \teBPF-based Networking, Security, and Observability\ncilium/cilium  \t1.11.5       \t1.11.5     \teBPF-based Networking, Security, and Observability\ncilium/cilium  \t1.11.4       \t1.11.4     \teBPF-based Networking, Security, and Observability\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSo we want \u003ccode\u003e1.11.4\u003c/code\u003e:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e❯ helm install cilium cilium/cilium --namespace kube-system --version 1.11.4\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNow our node should be ready!\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e❯ kubectl get node\nNAME           STATUS   ROLES                  AGE   VERSION\ncontrolplane   Ready    control-plane,master   24m   v1.23.5\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTime to join our worker to the cluster!\u003c/p\u003e\n\u003ch2\u003eJoining a worker to the cluster!\u003c/h2\u003e\n\u003cp\u003eWe have to go through the same steps as the controlplane to get the point that we have a\ncontainer runtime and \u003ccode\u003ekubeadm\u003c/code\u003e.   I’m not going to talk about the commands a second time but\nI’ll re-iterate them here for ease of following along.\u003c/p\u003e\n\u003cp\u003eFirst open up another shell and connect to the worker:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e❯ multipass shell worker\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNow run the following commands:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e❯ \u003cspan class=\"hljs-built_in\"\u003eecho\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e\"deb  http://apt.kubernetes.io/  kubernetes-xenial  main\"\u003c/span\u003e | sudo \u003cspan class=\"hljs-built_in\"\u003etee\u003c/span\u003e /etc/apt/sources.list.d/kubernetes.list\n❯ curl -fsSL  https://packages.cloud.google.com/apt/doc/apt-key.gpg|sudo gpg --dearmor -o /etc/apt/trusted.gpg.d/k8s.gpg\n❯ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/trusted.gpg.d/docker.gpg\n❯ \u003cspan class=\"hljs-built_in\"\u003eecho\u003c/span\u003e \u003cspan class=\"hljs-string\"\u003e\"deb https://download.docker.com/linux/ubuntu \u003cspan class=\"hljs-subst\"\u003e$(lsb_release -cs)\u003c/span\u003e stable\"\u003c/span\u003e | sudo \u003cspan class=\"hljs-built_in\"\u003etee\u003c/span\u003e /etc/apt/sources.list.d/docker.list\n\n❯ \u003cspan class=\"hljs-built_in\"\u003ecat\u003c/span\u003e \u0026#x3C;\u0026#x3C;\u003cspan class=\"hljs-string\"\u003eEOF | sudo tee /etc/modules-load.d/k8s.conf\noverlay\nbr_netfilter\nEOF\u003c/span\u003e\n\n❯ sudo sed -i -e \u003cspan class=\"hljs-string\"\u003e'/net.ipv4.conf.*.rp_filter/d'\u003c/span\u003e $(grep -ril \u003cspan class=\"hljs-string\"\u003e'\\.rp_filter'\u003c/span\u003e /etc/sysctl.d/ /usr/lib/sysctl.d/)\n❯ sudo sysctl -a | grep \u003cspan class=\"hljs-string\"\u003e'\\.rp_filter'\u003c/span\u003e | awk \u003cspan class=\"hljs-string\"\u003e'{print $1\" = 0\"}'\u003c/span\u003e | sudo \u003cspan class=\"hljs-built_in\"\u003etee\u003c/span\u003e -a /etc/sysctl.d/1000-cilium.conf\n\n❯ \u003cspan class=\"hljs-built_in\"\u003ecat\u003c/span\u003e \u0026#x3C;\u0026#x3C;\u003cspan class=\"hljs-string\"\u003eEOF | sudo tee /etc/sysctl.d/k8s.conf\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables  = 1\nnet.ipv4.ip_forward                 = 1\nEOF\u003c/span\u003e\n\n❯ sudo systemctl restart systemd-modules-load\n❯ sudo sysctl --system\n\n❯ sudo apt-get update \u0026#x26;\u0026#x26; sudo apt-get upgrade -y\n❯ sudo apt-get install containerd.io=1.6.8-1 -y\n\n❯ \u003cspan class=\"hljs-built_in\"\u003ecat\u003c/span\u003e \u0026#x3C;\u0026#x3C;\u003cspan class=\"hljs-string\"\u003eEOF | sudo tee /etc/containerd/config.toml\nversion = 2\n[plugins]\n  [plugins.\"io.containerd.grpc.v1.cri\"]\n   [plugins.\"io.containerd.grpc.v1.cri\".containerd]\n      [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes]\n        [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc]\n          runtime_type = \"io.containerd.runc.v2\"\n          [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc.options]\n            SystemdCgroup = true\nEOF\u003c/span\u003e\n\n❯ sudo systemctl restart containerd.service\n❯ sudo apt-get install kubeadm=1.23.5-00 kubelet=1.23.5-00 kubectl=1.23.5-00 -y\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFrom there we should be ready to join the cluster.   When we ran \u003ccode\u003ekubeadm init\u003c/code\u003e previously it\nprinted a join command out that we could use but I’m going to show you how to do it if you\nwere coming back later and no longer had that token.\u003c/p\u003e\n\u003cp\u003eBack on the \u003cem\u003econtroplane\u003c/em\u003e node run:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e❯ kubeadm token create --print-join-command\nkubeadm \u003cspan class=\"hljs-built_in\"\u003ejoin\u003c/span\u003e 192.168.64.7:6443 --token wxs197.cco6mjj9ricvu8ov --discovery-token-ca-cert-hash sha256:bd01c065240fa76f30a02ecb70a8cea6e329c9678994d4da1f6ccac7694b97fb\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNow copy that command and run it with \u003ccode\u003esudo\u003c/code\u003e on the worker:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e❯ sudo kubeadm \u003cspan class=\"hljs-built_in\"\u003ejoin\u003c/span\u003e 192.168.64.7:6443 --token wxs197.cco6mjj9ricvu8ov --discovery-token-ca-cert-hash sha256:bd01c065240fa76f30a02ecb70a8cea6e329c9678994d4da1f6ccac7694b97fb\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAfter this completes it’ll take a minute or two for everything to be synced up but if you go\nback to the master node you should have 2 ready nodes now:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e❯ kubectl get nodes\nNAME           STATUS   ROLES                  AGE   VERSION\ncontrolplane   Ready    control-plane,master   46m   v1.23.5\nworker         Ready    \u0026#x3C;none\u003e                 79s   v1.23.5\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003eAccessing the cluster outside of the VMs!\u003c/h2\u003e\n\u003cp\u003eNow the final part is to get the \u003ccode\u003eadmin.conf\u003c/code\u003e as a kubeconfig on your machine so you can control\nit from outside of the cluster.   To do this we can use scp\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003emultipass transfer controlplane:/home/ubuntu/.kube/config local.config\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNormally kubernetes configuration is in ~/.kube/config but I like to maint a separate file for\neach cluster and then I set the \u003ccode\u003eKUBECONFIG\u003c/code\u003e env var to access it.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"hljs language-bash\"\u003e❯ \u003cspan class=\"hljs-built_in\"\u003eexport\u003c/span\u003e KUBECONFIG=local.config\n❯ kubectl get nodes\nNAME           STATUS   ROLES                  AGE   VERSION\ncontrolplane   Ready    control-plane,master   56m   v1.23.5\nworker         Ready    \u0026#x3C;none\u003e                 11m   v1.23.5\n\u003c/code\u003e\u003c/pre\u003e","category":"Kubernetes","date":"2022-04-16T20:00:00-04:00","tags":["Linux","Kubernetes","DevOps","SRE"],"title":"Running a kubernetes cluster locally with kubeadm"}},"__N_SSG":true},"page":"/blog/[...id]","query":{"id":["2022","local_kubeadm_cluster"]},"buildId":"uPL-WT8At1JvJM6WotaJL","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>