{"pageProps":{"postData":{"id":["2022","learning_spanish"],"path":"2022/learning_spanish","contentHtml":"<p>I've been living in Puerto Rico for 4 years but two of those have been COVID and so I haven't been able to practice Spanish as much as I'd like. So to speed up my learning I've decided I want to watch a lot of spanish speaking television to start training my ears, but to do this I need a baseline of words I understand to be able to even know what they are saying!</p>\n<p>Learning through apps like Duolingo, Drops, etc start with weird topics like vegetables that don't get you to a very good baseline for actually understanding daily conversations, so I think consuming TV is a better use of my time.</p>\n<h2>Subtitles</h2>\n<p>I've decided the way to understand what the best words to study are is to download every subtitle for every episode of a show I want to watch and then count each word.  The more a word is spoken the more important it is for me to know it since I'll be hearing it a lot in the show.</p>\n<p>I'm going to download subtitles from Netflix. Subtitles in Netflix are in WebVTT format, which looks like this:</p>\n<div data-rehype-pretty-code-fragment=\"\"><pre class=\"github-dark-dimmed\" style=\"background-color: #22272e\" tabindex=\"0\" data-language=\"\" data-theme=\"default\"><code data-language=\"\" data-theme=\"default\" style=\"display: grid;\"><span data-line=\"\"><span style=\"color: #adbac7\">248</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">00:17:58.285 --> 00:18:01.163  position:50.00%,middle  align:middle size:80.00%  line:79.33% </span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">Yo de verdad espero que ustedes</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">me vean como una amiga, ¿mmm?</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\"></span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">249</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">00:18:01.247 --> 00:18:02.539  position:50.00%,middle  align:middle size:80.00%  line:84.67% </span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">No como una madrastra.</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\"></span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">250</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">00:18:04.250 --> 00:18:06.127  position:50.00%,middle  align:middle size:80.00%  line:84.67% </span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">Yo nunca te vi como una madrastra.</span></span></code></pre></div>\n<p>It gives you a start time, end time, and the text on the screen.   So my first process was parsing this format and just turning it into a list of words using https://github.com/glut23/webvtt-py.</p>\n<h3>Dummy parsing</h3>\n<p>What I basically did was <code>text.split(\" \")</code> and started counting the words.   This approach was quick and painless but it had a few downs falls.    Some words <em>look</em> the same when in reality they are not and so this meant I'd have to study every meaning of a word even if it was more rare.</p>\n<p>An example of this is the word \"como\", you can say:</p>\n<ul>\n<li>Haz como te digo: \"Do as I say\", where como means \"as\"</li>\n<li>como tacos todos los dias: \"I eat tacos every day\", where como is a conjugated form of the verb \"to eat\"</li>\n</ul>\n<p>I need to know which version of a word is being used so I can count it properly.</p>\n<h3>Regular Expressions are always the answer</h3>\n<p>I couldn't figure out what the word was without it being in a complete sentence, but subtitles are fragments.   They are split up into timings for displaying on the screen but they don't include entire sentences.  For example, it might look like this:</p>\n<div data-rehype-pretty-code-fragment=\"\"><pre class=\"github-dark-dimmed\" style=\"background-color: #22272e\" tabindex=\"0\" data-language=\"\" data-theme=\"default\"><code data-language=\"\" data-theme=\"default\" style=\"display: grid;\"><span data-line=\"\"><span style=\"color: #adbac7\">23</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">00:01:21.960 --> 00:01:23.520  position:50.00%,middle  align:middle size:80.00%  line:84.67% </span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">Solo las que luchan por ellos</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\"></span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">24</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">00:01:23.680 --> 00:01:25.680  position:50.00%,middle  align:middle size:80.00%  line:84.67% </span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">consiguen sus sueños.</span></span></code></pre></div>\n<p>I want to detect the start of a sentence and the end of a sentence and then combine it, so that you end up with \"Solo las que luchan por ellos consiguen sus sueños.\".   My first thought was a regular expression on punctuation.   This worked well <em>most</em> of the time but there were enough exceptions to the rule that it broke often on generated a lot of broken sentences:</p>\n<ul>\n<li>Abbreviations like \"EE. UU\" for estados unidos (united states)</li>\n<li>Ellipsis</li>\n</ul>\n<p>Splitting on spaces also didn't work for identifying the parts of speech since I needed the context around the word.</p>\n<center>\n<img src=\"/images/posts/learning_spanish/regex-extraction.png\">\n</center>\n<h2>Natural Language Processing</h2>\n<p>So to solve my pain I decided to grab https://spacy.io/ and do some NLP on the subtitles so that I could identify the proper parts of speech and get an accurate representation of the words I needed to learn.</p>\n<p>The way spaCy works is you can send it a sentence and it'll return you a set of tokens:</p>\n<div data-rehype-pretty-code-fragment=\"\"><pre class=\"github-dark-dimmed\" style=\"background-color: #22272e\" tabindex=\"0\" data-language=\"\" data-theme=\"default\"><code data-language=\"\" data-theme=\"default\" style=\"display: grid;\"><span data-line=\"\"><span style=\"color: #adbac7\">>>> import spacy</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">>>> nlp = spacy.load(\"es_core_news_sm\")</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">>>> [x.pos_ for x in nlp(\"Hola, como estas?\")]</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">['PROPN', 'PUNCT', 'SCONJ', 'PRON', 'PUNCT']</span></span></code></pre></div>\n<p>So now I could identify the parts of speech and pull sentences together through end of sentence punctation.   The first thing I did was generate a CSV of sentences that looked like this:</p>\n<table>\n<tbody><tr>\n<th>sentence</th>\n<th>start</th>\n<th>end</th>\n<th>show</th>\n<th>file</th>\n</tr>\n<tr>\n<td>Si no, le voy a cortar todos los deditos</td>\n<td>00:00:20.605</td>\n<td>00:00:24.125</td>\n<td>El marginal</td>\n<td>El marginal S02E02 WEBRip Netflix es[cc].vtt</td>\n</tr>\n</tbody></table>\n<p>Once I had a CSV of sentences I could send those back through spaCy for NLP and then start counting words, to generate another CSV:</p>\n<table>\n<tbody><tr>\n<th>word</th>\n<th>pos</th>\n<th>show</th>\n<th>file</th>\n</tr>\n<tr>\n<td>a</td>\n<td>ADP</td>\n<td>El marginal</td>\n<td>El marginal S02E02 WEBRip Netflix es[cc].vtt</td>\n</tr>\n<tr>\n<td>cortar</td>\n<td>VERB</td>\n<td>El marginal</td>\n<td>El marginal S02E02 WEBRip Netflix es[cc].vtt</td>\n</tr>\n<tr>\n<td>todos</td>\n<td>PRON</td>\n<td>El marginal</td>\n<td>El marginal S02E02 WEBRip Netflix es[cc].vtt</td>\n</tr>\n</tbody></table>\n<p>From there I had all the data I needed!   So now it was time to start doing some data analysis!</p>\n<h2>Data analysis</h2>\n<p>Using a jupyter notebook ( https://jupyter.org/ ) I grabbed pandas ( https://pandas.pydata.org/ ) and read in my CSVs to start analyzing the results.</p>\n<div data-rehype-pretty-code-fragment=\"\"><pre class=\"github-dark-dimmed\" style=\"background-color: #22272e\" tabindex=\"0\" data-language=\"\" data-theme=\"default\"><code data-language=\"\" data-theme=\"default\" style=\"display: grid;\"><span data-line=\"\"><span style=\"color: #adbac7\">import numpy as np</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">import pandas as pd</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">import matplotlib.pyplot as plt</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">pd.set_option('display.max_rows', 1000)</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">words = pd.read_csv('word_data.csv.gz', compression='gzip', delimiter=',')</span></span></code></pre></div>\n<p>The words dataframe is built up out of the second table I showed above with just words and their parts of speech.   I started off grouping the dataset by the word so I could get a count for how many times it was spoken in every series I parsed:</p>\n<div data-rehype-pretty-code-fragment=\"\"><pre class=\"github-dark-dimmed\" style=\"background-color: #22272e\" tabindex=\"0\" data-language=\"\" data-theme=\"default\"><code data-language=\"\" data-theme=\"default\" style=\"display: grid;\"><span data-line=\"\"><span style=\"color: #adbac7\">grouped_result = (words.groupby(words.word).size() </span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">   .sort_values(ascending=False) </span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">   .reset_index(name='count')</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">   .drop_duplicates(subset='word'))</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\"></span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">grouped_result.head(300)</span></span></code></pre></div>\n<p>Which returned a list of words and their count:</p>\n<div data-rehype-pretty-code-fragment=\"\"><pre class=\"github-dark-dimmed\" style=\"background-color: #22272e\" tabindex=\"0\" data-language=\"\" data-theme=\"default\"><code data-language=\"\" data-theme=\"default\" style=\"display: grid;\"><span data-line=\"\"><span style=\"color: #adbac7\">\tword\tcount</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">0\tque\t94430</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">1\tno\t75931</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">2\ta\t70968</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">3\tde\t67982</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">4\tser\t64226</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">5\tla\t52143</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">6\ty\t44390</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">7\testar\t37819</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">8\tel\t35920</span></span></code></pre></div>\n<p>Now I wanted to identify where my diminishing returns would be.   Is there a set of words that I must learn because they are spoken so often that I wouldn't understand a conversation if they weren't in my vocabulary?</p>\n<center>\n<img src=\"/images/posts/learning_spanish/diminishing_returns.png\">\n</center>\n<p>As you can see in this chart, the usage for words drops off at around the ~200 mark.   So there are basically 150 words I <em>must</em> know and then the rest are equally important.   I wasn't quite happy with this because some parts of speech are higher priority than others, for example I think having a strong understanding of the popular verbs will go a long way.  So I also wanted to identify what are the most important verbs to learn:</p>\n<div data-rehype-pretty-code-fragment=\"\"><pre class=\"github-dark-dimmed\" style=\"background-color: #22272e\" tabindex=\"0\" data-language=\"\" data-theme=\"default\"><code data-language=\"\" data-theme=\"default\" style=\"display: grid;\"><span data-line=\"\"><span style=\"color: #adbac7\">grouped_verbs = (words[words.pos == 'VERB'].groupby(['word', 'pos']).size() </span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">   .sort_values(ascending=False) </span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">   .reset_index(name='count')</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">   .drop_duplicates(subset='word'))</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\"></span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">grouped_verbs.head(50)</span></span></code></pre></div>\n<p>Which got me this:</p>\n<div data-rehype-pretty-code-fragment=\"\"><pre class=\"github-dark-dimmed\" style=\"background-color: #22272e\" tabindex=\"0\" data-language=\"\" data-theme=\"default\"><code data-language=\"\" data-theme=\"default\" style=\"display: grid;\"><span data-line=\"\"><span style=\"color: #adbac7\">\tword\tpos\tcount</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">0\ttener\tVERB\t22072</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">1\thacer\tVERB\t14946</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">2\tir\tVERB\t12570</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">3\tdecir\tVERB\t11314</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">4\tquerer\tVERB\t11083</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">5\tver\tVERB\t10269</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">6\testar\tVERB\t9780</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">7\tsaber\tVERB\t8704</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">8\tser\tVERB\t7674</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">9\tdar\tVERB\t5722</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">10\tpasar\tVERB\t5528</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">11\thablar\tVERB\t5355</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">12\tvenir\tVERB\t5145</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">13\tcreer\tVERB\t4895</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">14\tsalir \tVERB\t3395</span></span></code></pre></div>\n<p>Verbs had a slightly different drop-off pattern when I targeted them directly:</p>\n<center>\n<img src=\"/images/posts/learning_spanish/diminishing_verbs.png\">\n</center>\n<p>I get a big bang for my buck by learning those top 40 verbs.   Nouns on the other hand are much more spread out and most are evenly distributed:</p>\n<div data-rehype-pretty-code-fragment=\"\"><pre class=\"github-dark-dimmed\" style=\"background-color: #22272e\" tabindex=\"0\" data-language=\"\" data-theme=\"default\"><code data-language=\"\" data-theme=\"default\" style=\"display: grid;\"><span data-line=\"\"><span style=\"color: #adbac7\">word\tpos\tcount</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">0\tgracias\tNOUN\t4676</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">1\tfavor\tNOUN\t4625</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">2\tseñor\tNOUN\t4116</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">3\tverdad\tNOUN\t3566</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">4\tvida\tNOUN\t2673</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">5\thombre\tNOUN\t2601</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">6\tmadre\tNOUN\t2597</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">7\tvez\tNOUN\t2537</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">8\ttiempo\tNOUN\t2492</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">9\thijo\tNOUN\t2215</span></span></code></pre></div>\n<center>\n<img src=\"/images/posts/learning_spanish/diminishing_nouns.png\">\n</center>\n<p>So then I thought to myself... How much of a show would I understand if I just learned these most important words?  So I started by excluding some of the easy parts of speech and focused on the most important:</p>\n<div data-rehype-pretty-code-fragment=\"\"><pre class=\"github-dark-dimmed\" style=\"background-color: #22272e\" tabindex=\"0\" data-language=\"\" data-theme=\"default\"><code data-language=\"\" data-theme=\"default\" style=\"display: grid;\"><span data-line=\"\"><span style=\"color: #adbac7\">find_important_words = (words[~words.pos.isin(['PRON', 'CONJ', 'ADP', 'ADV', 'SCONJ', 'AUX', 'INTJ'])].groupby(['word', 'pos']).size() </span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">   .sort_values(ascending=False) </span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">   .reset_index(name='count')</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">   .drop_duplicates(subset='word'))</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\"></span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">find_important_words.head(50)</span></span></code></pre></div>\n<p>The top 20 were all verbs except for <code>bueno</code> and <code>gracias</code>.   So now with my list of what I considered \"important words\" I plotted it to find what amount of words I wanted to learn:</p>\n<center>\n<img src=\"/images/posts/learning_spanish/important_words.png\">\n</center>\n<p>It looks like 200 learned words would give me a reasonable amount of understanding for a series, so I decided to calculate how much of a series I would understand if I learned just those first 200 words:</p>\n<div data-rehype-pretty-code-fragment=\"\"><pre class=\"github-dark-dimmed\" style=\"background-color: #22272e\" tabindex=\"0\" data-language=\"\" data-theme=\"default\"><code data-language=\"\" data-theme=\"default\" style=\"display: grid;\"><span data-line=\"\"><span style=\"color: #adbac7\">percentages = {}</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\"></span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">for show_name in words['media'].drop_duplicates().values:</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">    words_in_show = (words[words.media == show_name].groupby(words.word).size() </span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">       .sort_values(ascending=False) </span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">       .reset_index(name='count')</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">       .drop_duplicates(subset='word'))</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">    </span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">    total_words_handled = 0</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\"></span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">    for word in grouped_result['word'][:200]:</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">        values = words_in_show[words_in_show.word == word]['count'].values</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\"></span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">        if values.size > 0:</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">            total_words_handled += values[0]</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\"></span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">    percentages[show_name] = total_words_handled / words_in_show.sum().loc['count']</span></span></code></pre></div>\n<p>Now I had a table that would show me what percentage of the spoken words were covered by the first 200 words in my list:</p>\n<div data-rehype-pretty-code-fragment=\"\"><pre class=\"github-dark-dimmed\" style=\"background-color: #22272e\" tabindex=\"0\" data-language=\"\" data-theme=\"default\"><code data-language=\"\" data-theme=\"default\" style=\"display: grid;\"><span data-line=\"\"><span style=\"color: #adbac7\">p_df = pd.DataFrame(percentages.items(), columns=['show', 'percentage'])</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">p_df = p_df.sort_values(by='percentage')</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">p_df['percentage'] = p_df['percentage'] * 100</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">pd.options.display.float_format = '{:,.2f}%'.format</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">p_df</span></span></code></pre></div>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<table>\n<tbody><tr>\n<th>Show</th>\n<th>Percentage</th>\n</tr><tr>\n<td>Verónica</td>\n<td>64.24%</td>\n</tr><tr>\n<td>El ciudadano ilustre</td>\n<td>65.28%</td>\n</tr><tr>\n<td>El Chapo</td>\n<td>66.68%</td>\n</tr><tr>\n<td>Neruda</td>\n<td>66.89%</td>\n</tr><tr>\n<td>La casa de papel</td>\n<td>67.56%</td>\n</tr><tr>\n<td>El Ministerio del Tiempo</td>\n<td>68.03%</td>\n</tr><tr>\n<td>Club de Cuervos</td>\n<td>68.19%</td>\n</tr><tr>\n<td>El marginal</td>\n<td>68.47%</td>\n</tr><tr>\n<td>Ingobernable</td>\n<td>68.59%</td>\n</tr><tr>\n<td>Pablo Escobar</td>\n<td>70.20%</td>\n</tr><tr>\n<td>Fariña</td>\n<td>70.95</td>\n</tr><tr>\n<td>La Reina del Sur</td>\n<td>71.52%</td>\n</tr><tr>\n<td>Gran Hotel</td>\n<td>73.15%</td>\n</tr><tr>\n<td>Las chicas del cable</td>\n<td>73.58%</td>\n</tr><tr>\n<td>Élite</td>\n<td>73.78%</td>\n</tr><tr>\n<td>La Piloto</td>\n<td>74.03%</td>\n</tr><tr>\n<td>El bar</td>\n<td>74.07%</td>\n</tr><tr>\n<td>La casa de las flores</td>\n<td>75.40%</td>\n</tr><tr>\n<td>Tarde para la ira</td>\n<td>75.59%</td>\n</tr></tbody></table>\n<p>But living in Puerto Rico, one thing I've realized is speed of speech is also important.  I have a much easier time speaking with people from Colombia and Mexico than I do with Puerto Ricans because they speak so much faster.   So even though I could understand 75% of \"Tarde para la ira\" if I learned the 200 words, I want to make sure they are speaking at a pace I could understand as well.</p>\n<p>So I loaded up the other CSV file that was the full sentences and added a \"time per word\" column:</p>\n<div data-rehype-pretty-code-fragment=\"\"><pre class=\"github-dark-dimmed\" style=\"background-color: #22272e\" tabindex=\"0\" data-language=\"\" data-theme=\"default\"><code data-language=\"\" data-theme=\"default\" style=\"display: grid;\"><span data-line=\"\"><span style=\"color: #adbac7\">sentences = pd.read_csv('sentences.csv.gz', compression='gzip', delimiter=',', parse_dates=['start', 'end'])</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">sentences['total_time'] = (sentences['end'] - sentences['start']).dt.total_seconds()</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">sentences['word_count'] = sentences['sentence'].str.split().str.len()</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">sentences['time_per_word'] = sentences['total_time'] / sentences['word_count']</span></span></code></pre></div>\n<p>Then I was able to have a speed rating for each show:</p>\n<div data-rehype-pretty-code-fragment=\"\"><pre class=\"github-dark-dimmed\" style=\"background-color: #22272e\" tabindex=\"0\" data-language=\"\" data-theme=\"default\"><code data-language=\"\" data-theme=\"default\" style=\"display: grid;\"><span data-line=\"\"><span style=\"color: #adbac7\">sentence_group = sentences.groupby([sentences.media])</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">sentence_group.time_per_word.mean().reset_index().sort_values('time_per_word')</span></span></code></pre></div>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<table>\n<tbody><tr>\n<th>media</th>\n<th>time_per_word</th>\n</tr><tr>\n<td>Gran Hotel</td>\n<td>0.58</td>\n</tr><tr>\n<td>El Chapo</td>\n<td>0.59</td>\n</tr><tr>\n<td>Las chicas del cable</td>\n<td>0.61</td>\n</tr><tr>\n<td>Élite</td>\n<td>0.63</td>\n</tr><tr>\n<td>Ingobernable</td>\n<td>0.64</td>\n</tr><tr>\n<td>El Ministerio del Tiempo</td>\n<td>0.64</td>\n</tr><tr>\n<td>Fariña</td>\n<td>0.65</td>\n</tr><tr>\n<td>El ciudadano ilustre</td>\n<td>0.67</td>\n</tr><tr>\n<td>Neruda</td>\n<td>0.68</td>\n</tr><tr>\n<td>La Piloto</td>\n<td>0.69</td>\n</tr><tr>\n<td>La casa de papel</td>\n<td>0.70</td>\n</tr><tr>\n<td>El bar</td>\n<td>0.70</td>\n</tr><tr>\n<td>Verónica</td>\n<td>0.72</td>\n</tr><tr>\n<td>La Reina del Sur</td>\n<td>0.75</td>\n</tr><tr>\n<td>Club de Cuervos</td>\n<td>0.76</td>\n</tr><tr>\n<td>El marginal</td>\n<td>0.76</td>\n</tr><tr>\n<td>Pablo Escobar</td>\n<td>0.77</td>\n</tr><tr>\n<td>Tarde para la ira</td>\n<td>0.77</td>\n</tr><tr>\n<td>La casa de las flores</td>\n<td>0.81</td>\n</tr></tbody></table>\n<p>Luckily the two series that have the least amount of vocabulary also speak the slowest!   So these will be the series I start with.    The final question I wanted to answer is \"What are the top words I'm missing for a series\".    Since I'll know 75% of the series from the top 200 words, I'm hoping there are some top words from a specific series that I can also learn to get an even higher understanding.</p>\n<p>First, find which words are in each show but not in the top 200:</p>\n<div data-rehype-pretty-code-fragment=\"\"><pre class=\"github-dark-dimmed\" style=\"background-color: #22272e\" tabindex=\"0\" data-language=\"\" data-theme=\"default\"><code data-language=\"\" data-theme=\"default\" style=\"display: grid;\"><span data-line=\"\"><span style=\"color: #adbac7\">missing_words_by_show = {}</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\"></span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">for show_name in words['media'].drop_duplicates().values:</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">    words_in_show = (words[words.media == show_name].groupby(words.word).size() </span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">       .sort_values(ascending=False) </span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">       .reset_index(name='count')</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">       .drop_duplicates(subset='word'))</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">    </span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">    frequency_words = grouped_result['word'][:200]</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\"></span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">    missing_words = words_in_show[~words_in_show.word.isin(frequency_words.values)]</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">    missing_words_by_show[show_name] = missing_words</span></span></code></pre></div>\n<p>Then we were able to grab them per show:</p>\n<div data-rehype-pretty-code-fragment=\"\"><pre class=\"github-dark-dimmed\" style=\"background-color: #22272e\" tabindex=\"0\" data-language=\"\" data-theme=\"default\"><code data-language=\"\" data-theme=\"default\" style=\"display: grid;\"><span data-line=\"\"><span style=\"color: #adbac7\">missing_words_by_show['La casa de las flores'].head(50)</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\"></span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">word\tcount</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">31\tmamá\t252</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">70\tflorería\t87</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">98\tperdón\t56</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">102\tsea\t54</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">116\tademás\t44</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">126\tahorita\t40</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">132\tcárcel\t38</span></span>\n<span data-line=\"\"><span style=\"color: #adbac7\">133\tfiesta\t38</span></span></code></pre></div>\n<p>So adding those few words to my vocabulary will also give me a better understanding of the series.</p>\n<h2>Conclusion</h2>\n<p>I believe a data-driven approach to language learning will be an effective way to get me speaking better spanish.   It was a ton of fun to play with spaCy, pandas, and jupyter as well!</p>\n<p>I'll improve the data analysis over time as well but I do believe this is a pretty good starting point!</p>\n<center>\n<img src=\"/images/posts/learning_spanish/meme.png\">\n</center>","category":"Development","date":"2022-04-29T20:00:00-04:00","tags":["Python","Pandas","NLP"],"title":"How to speak spanish like a colombian drug lord!"}},"__N_SSG":true}